{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCE4t1abRj4K"
   },
   "source": [
    "# Dataset\n",
    "Getting dataset from: [Benchmark Arabic text diacritization dataset](https://github.com/AliOsm/arabic-text-diacritization/tree/master)\n",
    "- train.txt: Contains 50,000 lines of diacritized Arabic text which can be used as training dataset\n",
    "- val.txt: Contains 2,500 lines of diacritized Arabic text which can be used as validation dataset\n",
    "- test.txt: Contains 2,500 lines of diacritized Arabic text which can be used as testing dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qqfHAZ0AYl-p"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/AliOsm/arabic-text-diacritization/refs/heads/master/dataset/train.txt &> /dev/null\n",
    "!wget https://raw.githubusercontent.com/AliOsm/arabic-text-diacritization/refs/heads/master/dataset/test.txt &> /dev/null\n",
    "!wget https://raw.githubusercontent.com/AliOsm/arabic-text-diacritization/refs/heads/master/dataset/val.txt &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "5E_4PVR0ZbZd"
   },
   "outputs": [],
   "source": [
    "def read_file_content(file_path):\n",
    "    return open(file_path, encoding=\"utf8\").read()\n",
    "\n",
    "# Read and split data based on lines\n",
    "train_data = read_file_content(\"/content/train.txt\").splitlines()\n",
    "val_data = read_file_content(\"/content/val.txt\").splitlines()\n",
    "test_data = read_file_content(\"/content/test.txt\").splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xi1xc9yBSqPJ"
   },
   "source": [
    "# Preprocessing dataset\n",
    "The Gemini API will act as the word sense disambugator module. It will provide the definition of each word based on it's context.\n",
    "The prompt that will be used:\n",
    "\n",
    "\n",
    "> Assume the role of an Arabic language expert, you know the definition of words in a given context.\n",
    "I'm going to provide you with a list of sentences, and for each sentence\n",
    "provide me a list of words and their word sense in arabic language and part of speech.\n",
    "Return the response as json\n",
    "List of sentences: [List of sentences]\n",
    "\n",
    "The dataset will be in this format\n",
    "```\n",
    "[{\n",
    "    \"sentence\": \"some text in arabic\",\n",
    "    \"words\": [\n",
    "      {\n",
    "        \"word\": \"word_1\",\n",
    "        \"word_sense\": \"definition_1\"\n",
    "        \"pos\" : \"part of speech\"\n",
    "      }\n",
    "    ]\n",
    "}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QsNHKHnFnvcC",
    "outputId": "932032e9-551d-4317-abad-d27b4a48580c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dzufOnf9ojaV",
    "outputId": "67f135af-c9ea-4190-c0a7-275df222fef5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/ATD-WSD\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/ATD-WSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "nLbkJK4havjz"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "-fWYnZbvbRFq"
   },
   "outputs": [],
   "source": [
    "# Import the Python SDK\n",
    "import google.generativeai as genai\n",
    "\n",
    "from google.colab import userdata\n",
    "\n",
    "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "Aw146LmBNCnC"
   },
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "MkC67LTwBHAk"
   },
   "outputs": [],
   "source": [
    "import typing_extensions as typing\n",
    "\n",
    "class Words(typing.TypedDict):\n",
    "    word: str\n",
    "    sense: str\n",
    "    pos: str\n",
    "\n",
    "class WSD(typing.TypedDict):\n",
    "    sentence: str\n",
    "    words: list[Words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3BNmgRouyYv1"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import time\n",
    "\n",
    "def extract_word_sense(prompt, name, is_start=True, is_end=False):\n",
    "  result = model.generate_content(\n",
    "      prompt,\n",
    "      generation_config=genai.GenerationConfig(\n",
    "          response_mime_type=\"application/json\", response_schema=list[WSD]\n",
    "      ),\n",
    "      safety_settings=[\n",
    "        {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "        {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "        {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
    "        {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"}\n",
    "      ]\n",
    "    )\n",
    "\n",
    "  with open(name, mode=\"a\") as f:\n",
    "    if is_start: f.write(\"[\")\n",
    "\n",
    "    f.write(result.text[1:-2])\n",
    "\n",
    "    if is_end: f.write(\"]\")\n",
    "    else: f.write(\",\")\n",
    "\n",
    "\n",
    "def prepare_dataset(data, data_start, data_end, name, batch_size=200):\n",
    "    file_name = f\"{name}_{data_end}.json\"\n",
    "\n",
    "    batch_data = data[data_start:data_end]\n",
    "\n",
    "    start = 0\n",
    "    end = batch_size\n",
    "\n",
    "    number_of_calls = math.ceil(len(batch_data)/batch_size)\n",
    "    for i in range(number_of_calls):\n",
    "      if i%15 == 0:\n",
    "        time.sleep(1)\n",
    "\n",
    "      prompt = f\"\"\"\n",
    "        You are an Arabic language expert, you understand the definition of words in a given context.\n",
    "        I will provide you with a list of sentences. For each sentence, please provide\n",
    "        a list of words, their part of speech and their word sense. Ignore numbers and punctuations\n",
    "        Return the response as json.\n",
    "        List of sentences:\n",
    "        {batch_data[start:end]}\n",
    "      \"\"\"\n",
    "      #extract_word_sense(prompt, name=file_name, is_start=(i==0), is_end=(i >= number_of_calls-1))\n",
    "      start = end\n",
    "      end = end + batch_size\n",
    "      if i%100 == 0:\n",
    "        print(f\"Current Batch:{data_end} \\| iteration: {i}/{number_of_calls}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "LG-F8a4wbsfq"
   },
   "outputs": [],
   "source": [
    "prepare_dataset(train_data, data_start=0, data_end=1000, name=\"train_wsd\", batch_size=2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
