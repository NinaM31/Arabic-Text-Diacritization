{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1- Setup Project"
      ],
      "metadata": {
        "id": "iTwVx-i4B_Dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-KshqcWmWrF",
        "outputId": "ad31a80b-1b7f-4cfc-b43d-634167603ac4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/ATD-WSD\n",
        "\n",
        "# Create dir to for storing trained model\n",
        "#!mkdir Baseline-w2v"
      ],
      "metadata": {
        "id": "PoBoeJn_mWn1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f572539-64cd-4ac7-bdab-79b1471fc41c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ATD-WSD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.1\n",
        "!pip install tensorflow==2.14.0"
      ],
      "metadata": {
        "id": "tch6Y_RWmWkx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "23a909e8-b3ce-4fc5-b4b7-166b7fd96d95"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.23.1\n",
            "  Using cached numpy-1.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "Using cached numpy-1.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.1.3\n",
            "    Uninstalling numpy-2.1.3:\n",
            "      Successfully uninstalled numpy-2.1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.23.1 which is incompatible.\n",
            "albumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.23.1 which is incompatible.\n",
            "bigframes 1.27.0 requires numpy>=1.24.0, but you have numpy 1.23.1 which is incompatible.\n",
            "chex 0.1.87 requires numpy>=1.24.1, but you have numpy 1.23.1 which is incompatible.\n",
            "ibis-framework 9.2.0 requires numpy<3,>=1.23.2, but you have numpy 1.23.1 which is incompatible.\n",
            "jax 0.4.33 requires numpy>=1.24, but you have numpy 1.23.1 which is incompatible.\n",
            "jaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.23.1 which is incompatible.\n",
            "mizani 0.13.0 requires numpy>=1.23.5, but you have numpy 1.23.1 which is incompatible.\n",
            "pandas-stubs 2.2.2.240909 requires numpy>=1.23.5, but you have numpy 1.23.1 which is incompatible.\n",
            "plotnine 0.14.1 requires numpy>=1.23.5, but you have numpy 1.23.1 which is incompatible.\n",
            "tensorflow 2.14.0 requires numpy>=1.23.5, but you have numpy 1.23.1 which is incompatible.\n",
            "tensorstore 0.1.68 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.14.0 which is incompatible.\n",
            "xarray 2024.10.0 requires numpy>=1.24, but you have numpy 1.23.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "5a4252a288ae4649a9bb998bd4a6019a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.14.0 in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (0.2.0)\n",
            "Collecting numpy>=1.23.5 (from tensorflow==2.14.0)\n",
            "  Using cached numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.14.0) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.14.0) (0.45.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.2.2)\n",
            "Using cached numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3070, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2863, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 447, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 578, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 247, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2786, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3072, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3082, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3135, in __init__\n",
            "    super().__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 43, in __init__\n",
            "    self.specifier: SpecifierSet = SpecifierSet(parsed.specifier)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/specifiers.py\", line 718, in __init__\n",
            "    self._specs = frozenset(map(Specifier, split_specifiers))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/specifiers.py\", line 330, in __hash__\n",
            "    def __hash__(self) -> int:\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1732, in isEnabledFor\n",
            "    return self._cache[level]\n",
            "KeyError: 50\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1523, in critical\n",
            "    if self.isEnabledFor(CRITICAL):\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1740, in isEnabledFor\n",
            "    level >= self.getEffectiveLevel()\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1719, in getEffectiveLevel\n",
            "    if logger.level:\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pickle\n",
        "import json\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import joblib\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from keras.layers import Embedding, Dense, Dropout, LSTM, Bidirectional, TimeDistributed, InputLayer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import Sequence\n",
        "from keras.initializers import glorot_normal\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from rich import print_json"
      ],
      "metadata": {
        "id": "pVmQYpAWmWhy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "    print(\"GPU device not found: working on CPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqoGoZ2VmWex",
        "outputId": "d4dd499a-4117-4f5b-c609-4122ef5f2f43"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU device not found: working on CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2- Importing Dataset\n",
        "The dataset that was prepared using Gemini which is 20% of the train, 100% val and testing taken from [original dataset](https://github.com/AliOsm/arabic-text-diacritization/tree/master/dataset)\n",
        "```\n",
        "[{\n",
        "    \"sentence\": \"some text in arabic\",\n",
        "    \"words\": [\n",
        "      {\n",
        "        \"word\": \"word_1\",\n",
        "        \"word_sense\": \"definition_1\"\n",
        "        \"pos\" : \"part_of_speech_1\"\n",
        "      }\n",
        "    ]\n",
        "}]\n",
        "```\n",
        "\n",
        "***⚠️This baseline model will use the sentences without the sense***"
      ],
      "metadata": {
        "id": "lklEh4uKCii9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helpers\n",
        "def read_json(file_path):\n",
        "  with open(file_path, mode=\"r\", encoding=\"utf-8\") as json_data:\n",
        "    return json.load(json_data)\n",
        "\n",
        "def get_sentences(data):\n",
        "  sentences = []\n",
        "  for s in data:\n",
        "    sentences.append(s['sentence'])\n",
        "  return sentences\n",
        "\n",
        "def pprint(json_data):\n",
        "  print_json(data=json_data, highlight=False)"
      ],
      "metadata": {
        "id": "Blt3y72LmWbr"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = read_json(\"/content/10485_train_wsd.json\")\n",
        "val_data = read_json(\"/content/2517_val_wsd.json\")\n",
        "train_data = train_data[:-5485]\n",
        "# val_data = val_data[:-2100]\n",
        "print('Training data length:', len(train_data))\n",
        "print(\"Train Sample\")\n",
        "pprint(train_data[100])\n",
        "\n",
        "print('Validation data length:', len(val_data))\n",
        "print(\"Val Sample\")\n",
        "pprint(val_data[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "id": "OwGB9ooSmWYT",
        "outputId": "04fa648b-4781-4fb4-ceea-1871a0708292"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data length: 5000\n",
            "Train Sample\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{\n",
              "  \"sentence\": \"فَاسِدٌ\",\n",
              "  \"words\": [\n",
              "    {\n",
              "      \"pos\": \"adjective\",\n",
              "      \"sense\": \"corrupt\",\n",
              "      \"word\": \"فَاسِدٌ\"\n",
              "    }\n",
              "  ]\n",
              "}\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">{\n",
              "  \"sentence\": \"فَاسِدٌ\",\n",
              "  \"words\": [\n",
              "    {\n",
              "      \"pos\": \"adjective\",\n",
              "      \"sense\": \"corrupt\",\n",
              "      \"word\": \"فَاسِدٌ\"\n",
              "    }\n",
              "  ]\n",
              "}\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation data length: 2517\n",
            "Val Sample\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{\n",
              "  \"sentence\": \"وَلَوْ لَمْ تَزِدْ( 26 / 106 )\",\n",
              "  \"words\": [\n",
              "    {\n",
              "      \"word\": \"وَلَوْ\",\n",
              "      \"pos\": \"conjunction\",\n",
              "      \"sense\": \"even if\"\n",
              "    },\n",
              "    {\n",
              "      \"word\": \"لَمْ\",\n",
              "      \"pos\": \"negative particle\",\n",
              "      \"sense\": \"not\"\n",
              "    },\n",
              "    {\n",
              "      \"word\": \"تَزِدْ\",\n",
              "      \"pos\": \"verb\",\n",
              "      \"sense\": \"to increase\"\n",
              "    }\n",
              "  ]\n",
              "}\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">{\n",
              "  \"sentence\": \"وَلَوْ لَمْ تَزِدْ( 26 / 106 )\",\n",
              "  \"words\": [\n",
              "    {\n",
              "      \"word\": \"وَلَوْ\",\n",
              "      \"pos\": \"conjunction\",\n",
              "      \"sense\": \"even if\"\n",
              "    },\n",
              "    {\n",
              "      \"word\": \"لَمْ\",\n",
              "      \"pos\": \"negative particle\",\n",
              "      \"sense\": \"not\"\n",
              "    },\n",
              "    {\n",
              "      \"word\": \"تَزِدْ\",\n",
              "      \"pos\": \"verb\",\n",
              "      \"sense\": \"to increase\"\n",
              "    }\n",
              "  ]\n",
              "}\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3- Constants"
      ],
      "metadata": {
        "id": "G5hK3htvKgvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helpers\n",
        "def CHAR_IDX(LIST):\n",
        "    char2idx = {}\n",
        "    idx2char = {}\n",
        "\n",
        "    for i, char in enumerate(LIST):\n",
        "        char2idx[char] = i\n",
        "        idx2char[i] = char\n",
        "\n",
        "    return char2idx, idx2char"
      ],
      "metadata": {
        "id": "9sdWVjt1Qv67"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ARABIC_CHAR = \"ىعظحرسيشضقثلصطكآماإهزءأفؤغجئدةخوبذتن\"\n",
        "NUMBERS = \"0123456789٠١٢٣٤٥٦٧٨٩\"\n",
        "\n",
        "# 15 possible Diacritics\n",
        "FATHATAN = u'\\u064b'\n",
        "DAMMATAN = u'\\u064c'\n",
        "KASRATAN = u'\\u064d'\n",
        "FATHA = u'\\u064e'\n",
        "DAMMA = u'\\u064f'\n",
        "KASRA = u'\\u0650'\n",
        "SHADDA = u'\\u0651'\n",
        "SUKUN = u'\\u0652'\n",
        "\n",
        "DIACRITICS = [\n",
        "    \"\",              # No Diacritic\n",
        "    FATHA,           # Fatha\n",
        "    FATHATAN,        # Fathatah\n",
        "    DAMMA,           # Damma\n",
        "    DAMMATAN,        # Dammatan\n",
        "    KASRA,           # Kasra\n",
        "    KASRATAN,        # Kasratan\n",
        "    SUKUN,           # Sukun\n",
        "    SHADDA,          # Shadda\n",
        "    SHADDA+FATHA,    # Shadda + Fatha\n",
        "    SHADDA+FATHATAN, # Shadda + Fathatah\n",
        "    SHADDA+DAMMA,    # Shadda + Damma\n",
        "    SHADDA+DAMMATAN, # Shadda + Dammatan\n",
        "    SHADDA+KASRA,    # Shadda + Kasra\n",
        "    SHADDA+KASRATAN  # Shadda + Kasratan\n",
        "]\n",
        "\n",
        "PUNCTUATIONS = [\n",
        "    \".\",    \"،\",    \":\",    \"؛\",\n",
        "    \"-\",    \"–\",    \"«\",    \"»\",\n",
        "    \"~\",    \"؟\",    \"!\",    \"*\",\n",
        "    \"(\",    \")\",    \"[\",    \"]\",\n",
        "    \"{\",    \"}\",    \";\",    \"\\n\",\n",
        "    \"'\",    \"\\\"\",   \"`\",    \"/\",\n",
        "    \",\",    \"?\",    '’',    '“',\n",
        "    '…',    '﴾',    '﴿',    \"+\",\n",
        "    \"*\",    \"=\",    \"&\",    \"_\",\n",
        "    \"\\n\",   \"\\u200d\",       \"\\u200f\"\n",
        "]\n",
        "\n",
        "\n",
        "# Special Tokens\n",
        "UNK_TOKEN = \"<unk>\"\n",
        "PAD_TOKEN = \"<pad>\"\n",
        "SOSS_TOKEN = \"<sense>\"\n",
        "EOSS_TOKEN = \"</sense>\"\n",
        "SOS_TOKEN = \"<s>\"\n",
        "EOS_TOKEN = \"</s>\"\n",
        "SPECIAL_TOKENS = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, SOSS_TOKEN, EOSS_TOKEN]\n",
        "\n",
        "# Combine\n",
        "ARABIC_CHAR_SPACE = list(ARABIC_CHAR) + [' ']\n",
        "ARABIC_CHAR_VALID = ARABIC_CHAR_SPACE + DIACRITICS\n",
        "ALLCHARS = ARABIC_CHAR_SPACE + list(NUMBERS) + PUNCTUATIONS + SPECIAL_TOKENS"
      ],
      "metadata": {
        "id": "smVTAeOjKggL"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_mapping, reverse_char_mapping = CHAR_IDX(ALLCHARS)\n",
        "class_mapping, reverse_class_mapping = CHAR_IDX(DIACRITICS)\n",
        "\n",
        "print(\"Char Mapping Size:\", len(char_mapping))\n",
        "print(\"Class Mapping Size:\", len(class_mapping))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4RjJXjyLw2e",
        "outputId": "baf58c49-4c5c-4566-f3e5-173b18aa7aec"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char Mapping Size: 99\n",
            "Class Mapping Size: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4- Helper Functions"
      ],
      "metadata": {
        "id": "Ge171xcVEiUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_diacritics_line(data):\n",
        "    return data.translate(str.maketrans('', '', ''.join(DIACRITICS)))\n",
        "\n",
        "def get_max_size(data):\n",
        "  return max(len(remove_diacritics_line(item['sentence'].strip())) for item in data)\n",
        "\n",
        "def get_min_size(data):\n",
        "  return min(len(remove_diacritics_line(item['sentence'].strip())) for item in data)\n",
        "\n",
        "def one_hot_matrix(data, size):\n",
        "    one_hot_matrix = [[1 if j == i else 0 for j in range(size)] for i in data]\n",
        "    return one_hot_matrix\n",
        "\n",
        "def one_hot_vector(index , size):\n",
        "    one_hot_vector = [1 if j == index else 0 for j in range(size)]\n",
        "    return one_hot_vector\n",
        "\n",
        "def get_words(l, line):\n",
        "  last_word = \"\"\n",
        "  first_word = \"\"\n",
        "  list_of_words = []\n",
        "\n",
        "  for i, w in enumerate(l.split()):\n",
        "    if i == 0:\n",
        "      first_word = w\n",
        "    if w not in PUNCTUATIONS:\n",
        "      last_word = w\n",
        "  found_first = False\n",
        "  if \"words\" not in line:\n",
        "    return {\"sentence\": l, \"words\": []}\n",
        "  for j, w in enumerate(line['words']):\n",
        "    if \"word\" not in w:\n",
        "      continue\n",
        "    if w['word'] == first_word:\n",
        "      found_first = True\n",
        "\n",
        "    if found_first:\n",
        "      list_of_words.append(w)\n",
        "      if w['word'] == last_word:\n",
        "        dic = {\"sentence\": l, \"words\": list_of_words}\n",
        "        return dic\n",
        "  dic = {\"sentence\": l, \"words\": list_of_words}\n",
        "  return dic\n",
        "\n",
        "def split_at(line, at='\\n'):\n",
        "  new_data = []\n",
        "  for l in line['sentence'].split(at):\n",
        "    new_data.append(get_words(l, line))\n",
        "  return new_data\n",
        "\n",
        "def punc_split(data):\n",
        "  new_data = []\n",
        "  for line in data:\n",
        "    line['sentence'] = line['sentence'].replace('.', '.\\n')\n",
        "    line['sentence'] = line['sentence'].replace(',', ',\\n')\n",
        "    line['sentence'] = line['sentence'].replace('،', '،\\n')\n",
        "    line['sentence'] = line['sentence'].replace(':', ':\\n')\n",
        "    line['sentence'] = line['sentence'].replace(';', ';\\n')\n",
        "    line['sentence'] = line['sentence'].replace('؛', '؛\\n')\n",
        "    line['sentence'] = line['sentence'].replace('(', '\\n\\(')\n",
        "    line['sentence'] = line['sentence'].replace(')', '\\)\\n')\n",
        "    line['sentence'] = line['sentence'].replace('[', '\\n[')\n",
        "    line['sentence'] = line['sentence'].replace(']', ']\\n')\n",
        "    line['sentence'] = line['sentence'].replace('{', '\\n{')\n",
        "    line['sentence'] = line['sentence'].replace('}', '}\\n')\n",
        "    line['sentence'] = line['sentence'].replace('«', '\\n«')\n",
        "    line['sentence'] = line['sentence'].replace('»', '»\\n')\n",
        "    line['sentence'] = line['sentence'].replace('؟', '؟\\n')\n",
        "    line['sentence'] = line['sentence'].replace('?', '?\\n')\n",
        "    line['sentence'] = line['sentence'].replace('!', '!\\n')\n",
        "    line['sentence'] = line['sentence'].replace('-', '-\\n')\n",
        "    for l in line['sentence'].split('\\n'):\n",
        "      a = get_words(l, line)\n",
        "      if a and len(a) > 0:\n",
        "        new_data.append(a)\n",
        "  return new_data\n",
        "\n",
        "def split_on_length(sentence_data, max_len=200):\n",
        "    split_sentences = []\n",
        "\n",
        "    for sentence in punc_split(sentence_data):\n",
        "       new_sentence = remove_diacritics_line(sentence['sentence'].strip())\n",
        "\n",
        "       if len(new_sentence) != 0:\n",
        "          if len(new_sentence) > 0 and len(new_sentence) <= max_len:\n",
        "                  #sentence['sentence'] = sentence['sentence'].strip()\n",
        "                  #split_sentences.append(sentence)\n",
        "                  dic = {\"sentence\": sentence['sentence'].strip(), \"words\": sentence['words']}\n",
        "                  split_sentences.append(dic)\n",
        "          else:\n",
        "            sentence_words = sentence['sentence'].split()\n",
        "            temp_sentence = ''\n",
        "\n",
        "            for word in sentence_words:\n",
        "              if len(remove_diacritics_line(temp_sentence).strip()) + len(remove_diacritics_line(word).strip()) + 1 > max_len:\n",
        "                  if len(remove_diacritics_line(temp_sentence).strip()) > 0:\n",
        "                      a = re.sub(temp_sentence, f'{temp_sentence}\\\\n', sentence['sentence'])\n",
        "                      dic = {\"sentence\": a.strip(), \"words\": sentence['words']}\n",
        "                      n = split_at(dic)\n",
        "                      for i in n:\n",
        "                        if len(i['sentence']) <= max_len:\n",
        "                          i['sentence'] = i['sentence'].strip()\n",
        "                          split_sentences.append(i)\n",
        "                  temp_sentence = word\n",
        "              else:\n",
        "                  temp_sentence = word if temp_sentence == '' else temp_sentence + ' ' + word\n",
        "\n",
        "            if len(remove_diacritics_line(temp_sentence).strip()) > 0:\n",
        "                  a = re.sub(temp_sentence, f'{temp_sentence}\\\\n', sentence['sentence'])\n",
        "                  dic = {\"sentence\": a.strip(), \"words\": sentence['words']}\n",
        "\n",
        "                  #sentence['sentence'] = re.sub(temp_sentence, f'{temp_sentence}\\\\n', sentence['sentence'])\n",
        "                  n = split_at(dic)\n",
        "                  for i in n:\n",
        "                    if len(i['sentence']) <= max_len:\n",
        "                      i['sentence'] = i['sentence'].strip()\n",
        "                      split_sentences.append(i)\n",
        "\n",
        "    return split_sentences\n",
        "def get_all_senses(docs):\n",
        "    sense = []\n",
        "    for doc in docs:\n",
        "      for word in doc['words']:\n",
        "        if 'sense' in word:\n",
        "          sense.append(word['sense'])\n",
        "    return sense\n",
        "\n",
        "def train_word_embeddings(docs):\n",
        "    doc = get_sentences(docs)\n",
        "    words = get_all_senses(docs)\n",
        "    doc.extend(words)\n",
        "    tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "    tokenizer.fit_on_texts(doc)\n",
        "\n",
        "    sentences = [dc.split() for dc in doc ]\n",
        "    sentences.append([UNK_TOKEN])\n",
        "    word2vec_model = Word2Vec(sentences, vector_size = 100, window=5, min_count=1, workers=4)\n",
        "\n",
        "    word_embeddings = word2vec_model.wv\n",
        "\n",
        "    return word_embeddings, tokenizer\n",
        "\n",
        "def get_word_embeddings(word):\n",
        "    encoded_docs = tokenizer.texts_to_sequences(word)\n",
        "    word_embeddings_for_sample = []\n",
        "    for word_index in encoded_docs:\n",
        "      if len(word_index) > 0:\n",
        "        if word_index[0] in data_embeddings:\n",
        "          word_embeddings_for_sample.append(data_embeddings[word_index[0]])\n",
        "    return word_embeddings_for_sample"
      ],
      "metadata": {
        "id": "pKp_49w2mWSl"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5- Prepare Data"
      ],
      "metadata": {
        "id": "3YTSfJP5oL6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_length_train_data      = split_on_length(train_data)\n",
        "split_length_val_data      = split_on_length(val_data)\n",
        "\n",
        "print(\"Train Data Size:\", len(split_length_train_data))\n",
        "print('Training data max:', get_max_size(split_length_train_data))\n",
        "print('Training data min:', get_min_size(split_length_train_data))\n",
        "print(\"Train Sample:\", split_length_train_data[0:1])\n",
        "print()\n",
        "\n",
        "print(\"Val Data Size:\", len(split_length_val_data))\n",
        "print('Validation data max:', get_max_size(split_length_val_data))\n",
        "print('Validation data min:', get_min_size(split_length_val_data))\n",
        "# print(\"Val Sample:\", split_length_val_data[0:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVqH68dByDKY",
        "outputId": "ca17bcd0-a2da-44f2-cece-41f99ca6b1b9"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data Size: 17103\n",
            "Training data max: 200\n",
            "Training data min: 1\n",
            "Train Sample: [{'sentence': 'وَلَوْ جَمَعَ ثُمَّ عَلِمَ تَرْكَ رُكْنٍ مِنْ الْأُولَى بَطَلَتَا وَيُعِيدُهُمَا جَامِعًا ،', 'words': [{'pos': 'conjunction', 'sense': 'even if', 'word': 'وَلَوْ'}, {'pos': 'verb', 'sense': 'to gather', 'word': 'جَمَعَ'}, {'pos': 'adverb', 'sense': 'then', 'word': 'ثُمَّ'}, {'pos': 'verb', 'sense': 'to know', 'word': 'عَلِمَ'}, {'pos': 'noun', 'sense': 'leaving', 'word': 'تَرْكَ'}, {'pos': 'noun', 'sense': 'pillar', 'word': 'رُكْنٍ'}, {'pos': 'preposition', 'sense': 'from', 'word': 'مِنْ'}, {'pos': 'adjective', 'sense': 'first', 'word': 'الْأُولَى'}, {'pos': 'verb', 'sense': 'to become invalid', 'word': 'بَطَلَتَا'}, {'pos': 'verb', 'sense': 'to return', 'word': 'وَيُعِيدُهُمَا'}, {'pos': 'adjective', 'sense': 'gathering', 'word': 'جَامِعًا'}]}]\n",
            "\n",
            "Val Data Size: 15413\n",
            "Validation data max: 200\n",
            "Validation data min: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data without diacritics and without punc and numbers"
      ],
      "metadata": {
        "id": "hCucn8BZ8nzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(data, remove_dia=False):\n",
        "  cleaned_data = []\n",
        "  for text in data:\n",
        "    temp = {'sentence': \"\", 'words':[]}\n",
        "    sen = text['sentence']\n",
        "    cleaned = ''.join(char for char in sen if char in ARABIC_CHAR_VALID)\n",
        "    cleaned = cleaned.strip()\n",
        "    if cleaned != \"\":\n",
        "      if remove_dia:\n",
        "        cleaned = remove_diacritics_line(cleaned)\n",
        "\n",
        "      temp['sentence'] = cleaned\n",
        "      temp['words'] = text['words']\n",
        "      cleaned_data.append(temp)\n",
        "  return cleaned_data\n",
        "\n",
        "\n",
        "clean_diac_train_data = clean_data(split_length_train_data)\n",
        "clean_diac_val_data = clean_data(split_length_val_data)\n",
        "\n",
        "print('Training data length:', len(clean_diac_train_data))\n",
        "print('Validation data length:', len(clean_diac_val_data))\n",
        "\n",
        "pprint(clean_diac_train_data[0:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hHz280qHoBuL",
        "outputId": "da5d5c35-7fd3-4187-ab7e-75f66b29462e"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data length: 12862\n",
            "Validation data length: 13403\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[\n",
              "  {\n",
              "    \"sentence\": \"وَلَوْ جَمَعَ ثُمَّ عَلِمَ تَرْكَ رُكْنٍ مِنْ الْأُولَى بَطَلَتَا وَيُعِيدُهُمَا جَامِعًا\",\n",
              "    \"words\": [\n",
              "      {\n",
              "        \"pos\": \"conjunction\",\n",
              "        \"sense\": \"even if\",\n",
              "        \"word\": \"وَلَوْ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"verb\",\n",
              "        \"sense\": \"to gather\",\n",
              "        \"word\": \"جَمَعَ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"adverb\",\n",
              "        \"sense\": \"then\",\n",
              "        \"word\": \"ثُمَّ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"verb\",\n",
              "        \"sense\": \"to know\",\n",
              "        \"word\": \"عَلِمَ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"noun\",\n",
              "        \"sense\": \"leaving\",\n",
              "        \"word\": \"تَرْكَ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"noun\",\n",
              "        \"sense\": \"pillar\",\n",
              "        \"word\": \"رُكْنٍ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"preposition\",\n",
              "        \"sense\": \"from\",\n",
              "        \"word\": \"مِنْ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"adjective\",\n",
              "        \"sense\": \"first\",\n",
              "        \"word\": \"الْأُولَى\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"verb\",\n",
              "        \"sense\": \"to become invalid\",\n",
              "        \"word\": \"بَطَلَتَا\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"verb\",\n",
              "        \"sense\": \"to return\",\n",
              "        \"word\": \"وَيُعِيدُهُمَا\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"adjective\",\n",
              "        \"sense\": \"gathering\",\n",
              "        \"word\": \"جَامِعًا\"\n",
              "      }\n",
              "    ]\n",
              "  },\n",
              "  {\n",
              "    \"sentence\": \"أَوْ مِنْ الثَّانِيَةِ\",\n",
              "    \"words\": [\n",
              "      {\n",
              "        \"pos\": \"conjunction\",\n",
              "        \"sense\": \"or\",\n",
              "        \"word\": \"أَوْ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"preposition\",\n",
              "        \"sense\": \"from\",\n",
              "        \"word\": \"مِنْ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"adjective\",\n",
              "        \"sense\": \"second\",\n",
              "        \"word\": \"الثَّانِيَةِ\"\n",
              "      }\n",
              "    ]\n",
              "  }\n",
              "]\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[\n",
              "  {\n",
              "    \"sentence\": \"وَلَوْ جَمَعَ ثُمَّ عَلِمَ تَرْكَ رُكْنٍ مِنْ الْأُولَى بَطَلَتَا وَيُعِيدُهُمَا جَامِعًا\",\n",
              "    \"words\": [\n",
              "      {\n",
              "        \"pos\": \"conjunction\",\n",
              "        \"sense\": \"even if\",\n",
              "        \"word\": \"وَلَوْ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"verb\",\n",
              "        \"sense\": \"to gather\",\n",
              "        \"word\": \"جَمَعَ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"adverb\",\n",
              "        \"sense\": \"then\",\n",
              "        \"word\": \"ثُمَّ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"verb\",\n",
              "        \"sense\": \"to know\",\n",
              "        \"word\": \"عَلِمَ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"noun\",\n",
              "        \"sense\": \"leaving\",\n",
              "        \"word\": \"تَرْكَ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"noun\",\n",
              "        \"sense\": \"pillar\",\n",
              "        \"word\": \"رُكْنٍ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"preposition\",\n",
              "        \"sense\": \"from\",\n",
              "        \"word\": \"مِنْ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"adjective\",\n",
              "        \"sense\": \"first\",\n",
              "        \"word\": \"الْأُولَى\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"verb\",\n",
              "        \"sense\": \"to become invalid\",\n",
              "        \"word\": \"بَطَلَتَا\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"verb\",\n",
              "        \"sense\": \"to return\",\n",
              "        \"word\": \"وَيُعِيدُهُمَا\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"adjective\",\n",
              "        \"sense\": \"gathering\",\n",
              "        \"word\": \"جَامِعًا\"\n",
              "      }\n",
              "    ]\n",
              "  },\n",
              "  {\n",
              "    \"sentence\": \"أَوْ مِنْ الثَّانِيَةِ\",\n",
              "    \"words\": [\n",
              "      {\n",
              "        \"pos\": \"conjunction\",\n",
              "        \"sense\": \"or\",\n",
              "        \"word\": \"أَوْ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"preposition\",\n",
              "        \"sense\": \"from\",\n",
              "        \"word\": \"مِنْ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"adjective\",\n",
              "        \"sense\": \"second\",\n",
              "        \"word\": \"الثَّانِيَةِ\"\n",
              "      }\n",
              "    ]\n",
              "  }\n",
              "]\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data with diacritics and without punc and numbers"
      ],
      "metadata": {
        "id": "SEPnoVft8dyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clean_train_data = [remove_diacritics_line(text) for text in clean_diac_train_data]\n",
        "# clean_val_data = [remove_diacritics_line(text) for text in clean_diac_val_data]\n",
        "\n",
        "clean_train_data = clean_data(split_length_train_data, remove_dia=True)\n",
        "clean_val_data = clean_data(split_length_val_data, remove_dia=True)\n",
        "\n",
        "print('Training data length:', len(clean_train_data))\n",
        "print('Validation data length:', len(clean_val_data))\n",
        "\n",
        "pprint(clean_train_data[0:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pQ6mag5FoBrb",
        "outputId": "c290ba0a-4a59-4fb0-d21c-e2c951561236"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data length: 12862\n",
            "Validation data length: 13403\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[\n",
              "  {\n",
              "    \"sentence\": \"ولو جمع ثم علم ترك ركن من الأولى بطلتا ويعيدهما جامعا\",\n",
              "    \"words\": [\n",
              "      {\n",
              "        \"pos\": \"conjunction\",\n",
              "        \"sense\": \"even if\",\n",
              "        \"word\": \"وَلَوْ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"verb\",\n",
              "        \"sense\": \"to gather\",\n",
              "        \"word\": \"جَمَعَ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"adverb\",\n",
              "        \"sense\": \"then\",\n",
              "        \"word\": \"ثُمَّ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"verb\",\n",
              "        \"sense\": \"to know\",\n",
              "        \"word\": \"عَلِمَ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"noun\",\n",
              "        \"sense\": \"leaving\",\n",
              "        \"word\": \"تَرْكَ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"noun\",\n",
              "        \"sense\": \"pillar\",\n",
              "        \"word\": \"رُكْنٍ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"preposition\",\n",
              "        \"sense\": \"from\",\n",
              "        \"word\": \"مِنْ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"adjective\",\n",
              "        \"sense\": \"first\",\n",
              "        \"word\": \"الْأُولَى\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"verb\",\n",
              "        \"sense\": \"to become invalid\",\n",
              "        \"word\": \"بَطَلَتَا\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"verb\",\n",
              "        \"sense\": \"to return\",\n",
              "        \"word\": \"وَيُعِيدُهُمَا\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"adjective\",\n",
              "        \"sense\": \"gathering\",\n",
              "        \"word\": \"جَامِعًا\"\n",
              "      }\n",
              "    ]\n",
              "  },\n",
              "  {\n",
              "    \"sentence\": \"أو من الثانية\",\n",
              "    \"words\": [\n",
              "      {\n",
              "        \"pos\": \"conjunction\",\n",
              "        \"sense\": \"or\",\n",
              "        \"word\": \"أَوْ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"preposition\",\n",
              "        \"sense\": \"from\",\n",
              "        \"word\": \"مِنْ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"adjective\",\n",
              "        \"sense\": \"second\",\n",
              "        \"word\": \"الثَّانِيَةِ\"\n",
              "      }\n",
              "    ]\n",
              "  }\n",
              "]\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">[\n",
              "  {\n",
              "    \"sentence\": \"ولو جمع ثم علم ترك ركن من الأولى بطلتا ويعيدهما جامعا\",\n",
              "    \"words\": [\n",
              "      {\n",
              "        \"pos\": \"conjunction\",\n",
              "        \"sense\": \"even if\",\n",
              "        \"word\": \"وَلَوْ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"verb\",\n",
              "        \"sense\": \"to gather\",\n",
              "        \"word\": \"جَمَعَ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"adverb\",\n",
              "        \"sense\": \"then\",\n",
              "        \"word\": \"ثُمَّ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"verb\",\n",
              "        \"sense\": \"to know\",\n",
              "        \"word\": \"عَلِمَ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"noun\",\n",
              "        \"sense\": \"leaving\",\n",
              "        \"word\": \"تَرْكَ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"noun\",\n",
              "        \"sense\": \"pillar\",\n",
              "        \"word\": \"رُكْنٍ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"preposition\",\n",
              "        \"sense\": \"from\",\n",
              "        \"word\": \"مِنْ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"adjective\",\n",
              "        \"sense\": \"first\",\n",
              "        \"word\": \"الْأُولَى\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"verb\",\n",
              "        \"sense\": \"to become invalid\",\n",
              "        \"word\": \"بَطَلَتَا\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"verb\",\n",
              "        \"sense\": \"to return\",\n",
              "        \"word\": \"وَيُعِيدُهُمَا\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"adjective\",\n",
              "        \"sense\": \"gathering\",\n",
              "        \"word\": \"جَامِعًا\"\n",
              "      }\n",
              "    ]\n",
              "  },\n",
              "  {\n",
              "    \"sentence\": \"أو من الثانية\",\n",
              "    \"words\": [\n",
              "      {\n",
              "        \"pos\": \"conjunction\",\n",
              "        \"sense\": \"or\",\n",
              "        \"word\": \"أَوْ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"preposition\",\n",
              "        \"sense\": \"from\",\n",
              "        \"word\": \"مِنْ\"\n",
              "      },\n",
              "      {\n",
              "        \"pos\": \"adjective\",\n",
              "        \"sense\": \"second\",\n",
              "        \"word\": \"الثَّانِيَةِ\"\n",
              "      }\n",
              "    ]\n",
              "  }\n",
              "]\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train word embedding"
      ],
      "metadata": {
        "id": "ET_nC50K86C_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_to_embeddings = clean_train_data + clean_val_data\n",
        "data_embeddings, tokenizer = train_word_embeddings(data_to_embeddings)"
      ],
      "metadata": {
        "id": "yP1XBrwooBmO"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6- Custom Data Generator"
      ],
      "metadata": {
        "id": "s8Q1H6ZKacWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_classes(sentence):\n",
        "  x = []\n",
        "  y = []\n",
        "\n",
        "  unk_emb = get_word_embeddings([UNK_TOKEN])[0]\n",
        "\n",
        "  vec = []\n",
        "  vec = one_hot_vector(char_mapping[SOS_TOKEN],len(char_mapping))\n",
        "  vec.extend(unk_emb)\n",
        "  x.append(vec)\n",
        "\n",
        "  y.append(one_hot_vector(class_mapping[''],len(class_mapping)))\n",
        "  split_sentence = [i for j in sentence['sentence'].split() for i in (j, ' ')][:-1]\n",
        "\n",
        "  for word in split_sentence:\n",
        "    emb2 = get_word_embeddings(remove_diacritics_line(word))\n",
        "\n",
        "    if (len(emb2) == 0):\n",
        "        emb = unk_emb\n",
        "    else:\n",
        "        emb = emb2[0]\n",
        "\n",
        "    if word in PUNCTUATIONS:\n",
        "      emb = unk_emb\n",
        "    else:\n",
        "      if (len(emb2) == 0):\n",
        "        emb = unk_emb\n",
        "      else:\n",
        "        emb = emb2[0]\n",
        "\n",
        "    for index, char in enumerate(word):\n",
        "      if char not in DIACRITICS:\n",
        "        vec = []\n",
        "        vec = one_hot_vector(char_mapping[char],len(char_mapping))\n",
        "        vec.extend(emb)\n",
        "        x.append(vec)\n",
        "        char_diacritic = ''\n",
        "        sentence_len = len(sentence['sentence'])\n",
        "\n",
        "        if index + 1 < sentence_len:\n",
        "          if sentence['sentence'][index + 1] in DIACRITICS:\n",
        "            char_diacritic = sentence['sentence'][index + 1]\n",
        "\n",
        "            if index + 2 < sentence_len:\n",
        "               char_diacritic = char_diacritic + sentence['sentence'][index + 2] if sentence['sentence'][index + 2] in DIACRITICS and (char_diacritic + sentence['sentence'][index + 2] in class_mapping) else sentence['sentence'][index + 2] + char_diacritic if sentence['sentence'][index + 2] in DIACRITICS and (sentence['sentence'][index + 2] + char_diacritic in class_mapping) else char_diacritic\n",
        "        y.append(one_hot_vector(class_mapping[char_diacritic],len(class_mapping)))\n",
        "\n",
        "  vec = []\n",
        "  vec = one_hot_vector(char_mapping[EOS_TOKEN],len(char_mapping))\n",
        "  vec.extend(unk_emb)\n",
        "  x.append(vec)\n",
        "  y.append(one_hot_vector(class_mapping[\"\"],len(class_mapping)))\n",
        "\n",
        "  emp_s_emb = get_word_embeddings([UNK_TOKEN])[0]\n",
        "  vec = []\n",
        "  vec = one_hot_vector(char_mapping[SOSS_TOKEN],len(char_mapping))\n",
        "  vec.extend(emp_s_emb)\n",
        "  x.append(vec)\n",
        "  y.append(one_hot_vector(class_mapping[\"\"],len(class_mapping)))\n",
        "\n",
        "  for word in split_sentence:\n",
        "    sens_emb2 = get_word_embeddings([UNK_TOKEN])\n",
        "    for s in sentence['words']:\n",
        "      if 'sense' in s:\n",
        "        if 'word' in s and s['word'] == word:\n",
        "          sens_emb2 = get_word_embeddings(s['sense'])\n",
        "\n",
        "      if (len(sens_emb2) == 0):\n",
        "          sens_emb = unk_emb\n",
        "      else:\n",
        "          sens_emb = sens_emb2[0]\n",
        "\n",
        "      if word in PUNCTUATIONS:\n",
        "        sens_emb = unk_emb\n",
        "      else:\n",
        "        if (len(sens_emb2) == 0):\n",
        "          sens_emb = unk_emb\n",
        "        else:\n",
        "          sens_emb = sens_emb2[0]\n",
        "\n",
        "      vec = []\n",
        "      vec = one_hot_vector(char_mapping[PAD_TOKEN], len(char_mapping))\n",
        "      vec.extend(sens_emb)\n",
        "      x.append(vec)\n",
        "      y.append(one_hot_vector(class_mapping[\"\"],len(class_mapping)))\n",
        "\n",
        "  vec = []\n",
        "  vec = one_hot_vector(char_mapping[EOSS_TOKEN],len(char_mapping))\n",
        "  vec.extend(emp_s_emb)\n",
        "  x.append(vec)\n",
        "  y.append(one_hot_vector(class_mapping[''],len(class_mapping)))\n",
        "\n",
        "  assert(len(x) == len(y))\n",
        "\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "b1dnZ-DeoBgd"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_classes(data):\n",
        "  X = []\n",
        "  Y = []\n",
        "\n",
        "  for sentence in data:\n",
        "    x, y = get_sentence_classes(sentence)\n",
        "    X.append(x)\n",
        "    Y.append(y)\n",
        "\n",
        "  X = np.asarray(X)\n",
        "  Y = np.asarray(Y)\n",
        "\n",
        "  return X, Y\n",
        "\n",
        "class custom_data_generator(Sequence):\n",
        "\n",
        "    def __init__(self, data, batch_size):\n",
        "        self.data = data\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.data) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        start_index = index * self.batch_size\n",
        "        end_index = (index + 1) * self.batch_size\n",
        "\n",
        "        batch = self.data[start_index : end_index]\n",
        "        X_batch, Y_batch = get_classes(batch)\n",
        "\n",
        "        max_length_X = np.max([len(x) for x in X_batch])\n",
        "        max_length_Y = np.max([len(y) for y in Y_batch])\n",
        "\n",
        "        assert(max_length_X == max_length_Y)\n",
        "\n",
        "        vec = []\n",
        "        vec = one_hot_vector(char_mapping[PAD_TOKEN],len(char_mapping))\n",
        "        vec.extend(get_word_embeddings([PAD_TOKEN])[0])\n",
        "\n",
        "        X = []\n",
        "        for x in X_batch:\n",
        "          padding_length = max_length_X - len(x)\n",
        "          x = list(x)\n",
        "          x.extend([vec] * (padding_length))\n",
        "          X.append(np.asarray(x))\n",
        "\n",
        "        Y = []\n",
        "        for y in Y_batch:\n",
        "          padding_length = max_length_Y - len(y)\n",
        "          y = list(y)\n",
        "\n",
        "          y.extend(one_hot_matrix([class_mapping['']] * (padding_length), len(class_mapping)))\n",
        "          Y.append(np.asarray(y))\n",
        "\n",
        "        X, Y = np.asarray(X), np.asarray(Y)\n",
        "        return X, Y"
      ],
      "metadata": {
        "id": "8K_GCQrRmWAs"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "   model = Sequential()\n",
        "   model.add(InputLayer(input_shape=(None, 199)))\n",
        "\n",
        "   model.add(Bidirectional(LSTM(units=64,return_sequences=True,kernel_initializer=glorot_normal(seed=500))))\n",
        "   model.add(Dropout(0.5))\n",
        "   model.add(Bidirectional(LSTM(units=64,return_sequences=True,kernel_initializer=glorot_normal(seed=500))))\n",
        "   model.add(Dropout(0.5))\n",
        "   model.add(Bidirectional(LSTM(units=128,return_sequences=True,kernel_initializer=glorot_normal(seed=500))))\n",
        "   #model.add(TimeDistributed(Dense(units=128,activation='relu',kernel_initializer=glorot_normal(seed=500))))\n",
        "   model.add(TimeDistributed(Dense(units=128,activation='relu',kernel_initializer=glorot_normal(seed=500))))\n",
        "   model.add(TimeDistributed(Dense(units=len(class_mapping),activation='softmax',kernel_initializer=glorot_normal(seed=500))))\n",
        "   model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "   return model"
      ],
      "metadata": {
        "id": "Y2soLOb3ohZx"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOMzl4wAohRq",
        "outputId": "5f31b6f2-d64c-4998-c1d6-6205ab34be2a"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_30 (Bidirect  (None, None, 128)         135168    \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dropout_20 (Dropout)        (None, None, 128)         0         \n",
            "                                                                 \n",
            " bidirectional_31 (Bidirect  (None, None, 128)         98816     \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dropout_21 (Dropout)        (None, None, 128)         0         \n",
            "                                                                 \n",
            " bidirectional_32 (Bidirect  (None, None, 256)         263168    \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " time_distributed_20 (TimeD  (None, None, 128)         32896     \n",
            " istributed)                                                     \n",
            "                                                                 \n",
            " time_distributed_21 (TimeD  (None, None, 15)          1935      \n",
            " istributed)                                                     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 531983 (2.03 MB)\n",
            "Trainable params: 531983 (2.03 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_model(model, epochs, batch_size, train_data, val_data):\n",
        "    random.shuffle(train_data)\n",
        "    random.shuffle(val_data)\n",
        "\n",
        "    checkpoint_path = '/content/drive/MyDrive/ATD-WSD/Bilstm-wsd/epoch{epoch:02d}.ckpt'\n",
        "\n",
        "    checkpoint_cb = ModelCheckpoint(checkpoint_path, verbose=0)\n",
        "\n",
        "    training_generator = custom_data_generator(train_data, batch_size)\n",
        "    val_generator = custom_data_generator(val_data, batch_size)\n",
        "\n",
        "    history =  model.fit(training_generator,validation_data=val_generator,epochs=epochs,callbacks=[checkpoint_cb])\n",
        "    return history"
      ],
      "metadata": {
        "id": "bgf8wJQvomsF"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = fit_model(model, 1, 32, clean_diac_train_data, clean_diac_val_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-w9KIkNcomoM",
        "outputId": "45f73415-8815-412c-afa1-a4f2e11b8a39"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-195-a60007efa8d2>:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.asarray(X)\n",
            "<ipython-input-195-a60007efa8d2>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  Y = np.asarray(Y)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "402/402 [==============================] - 10350s 26s/step - loss: 0.0645 - accuracy: 0.9871 - val_loss: 0.0257 - val_accuracy: 0.9933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7- Checkpoint"
      ],
      "metadata": {
        "id": "ewXj0HZCKjMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/ATD-WSD\n",
        "joblib.dump(model, 'bilstmWSD.joblib')\n",
        "filename = 'bilstmWSD.sav'\n",
        "pickle.dump(model, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "mpKJRuWNomfK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1141484-eed4-49cf-c42c-4cf7caabdc4e"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ATD-WSD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8- Predict"
      ],
      "metadata": {
        "id": "0u5_tScRK3_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(line, model):\n",
        "    dic = {\"sentence\": remove_diacritics_line(line['sentence']), \"words\": line['words']}\n",
        "    #line['sentence'] = remove_diacritics_line(line['sentence'])\n",
        "\n",
        "    X, _ = get_classes([dic])\n",
        "    predictions = model.predict(X).squeeze()\n",
        "\n",
        "    output = ''\n",
        "    for char, prediction in zip(line['sentence'], predictions):\n",
        "        output += char\n",
        "        if char not in ARABIC_CHAR:\n",
        "            continue\n",
        "        output += reverse_class_mapping[np.argmax(prediction)]\n",
        "    return output\n",
        "\n",
        "def predict_text(data, model, file_name):\n",
        "  for idx, line in enumerate(data):\n",
        "    output = predict(line, model)\n",
        "    with open(f\"{file_name}_out.txt\", 'a') as file:\n",
        "      file.write(output + \"\\n\")\n",
        "\n",
        "    with open(f\"{file_name}_inp.txt\", 'a') as file:\n",
        "      file.write(line['sentence'] + \"\\n\")"
      ],
      "metadata": {
        "id": "2pfEp6gNbCcU"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_file_path = 'bilstmWSD.joblib'\n",
        "model = joblib.load(model_file_path)"
      ],
      "metadata": {
        "id": "meOKesGJmV5l"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = read_json(\"/content/2528_test_wsd.json\")\n",
        "\n",
        "print('Testing data length:', len(test_data))\n",
        "print(\"Test Sample\")\n",
        "pprint(test_data[100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "J1H_mVk4yS_H",
        "outputId": "af287b06-64df-4088-968b-5375859a00c6"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing data length: 2528\n",
            "Test Sample\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{\n",
              "  \"sentence\": \"قَوْلُهُ لَمْ يَجُزْ لَهُ التَّيَمُّمُ ) يُتَأَمَّلُ وَجْهُ ذَلِكَ .\",\n",
              "  \"words\": [\n",
              "    {\n",
              "      \"pos\": \"noun\",\n",
              "      \"sense\": \"saying\",\n",
              "      \"word\": \"قَوْلُهُ\"\n",
              "    },\n",
              "    {\n",
              "      \"pos\": \"verb\",\n",
              "      \"sense\": \"not permitted\",\n",
              "      \"word\": \"لَمْ يَجُزْ\"\n",
              "    },\n",
              "    {\n",
              "      \"pos\": \"preposition\",\n",
              "      \"sense\": \"for\",\n",
              "      \"word\": \"لَهُ\"\n",
              "    },\n",
              "    {\n",
              "      \"pos\": \"noun\",\n",
              "      \"sense\": \"dry ablution\",\n",
              "      \"word\": \"التَّيَمُّمُ\"\n",
              "    },\n",
              "    {\n",
              "      \"pos\": \"verb\",\n",
              "      \"sense\": \"to be contemplated\",\n",
              "      \"word\": \"يُتَأَمَّلُ\"\n",
              "    },\n",
              "    {\n",
              "      \"pos\": \"noun\",\n",
              "      \"sense\": \"face\",\n",
              "      \"word\": \"وَجْهُ\"\n",
              "    },\n",
              "    {\n",
              "      \"pos\": \"pronoun\",\n",
              "      \"sense\": \"that\",\n",
              "      \"word\": \"ذَلِكَ\"\n",
              "    }\n",
              "  ]\n",
              "}\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">{\n",
              "  \"sentence\": \"قَوْلُهُ لَمْ يَجُزْ لَهُ التَّيَمُّمُ ) يُتَأَمَّلُ وَجْهُ ذَلِكَ .\",\n",
              "  \"words\": [\n",
              "    {\n",
              "      \"pos\": \"noun\",\n",
              "      \"sense\": \"saying\",\n",
              "      \"word\": \"قَوْلُهُ\"\n",
              "    },\n",
              "    {\n",
              "      \"pos\": \"verb\",\n",
              "      \"sense\": \"not permitted\",\n",
              "      \"word\": \"لَمْ يَجُزْ\"\n",
              "    },\n",
              "    {\n",
              "      \"pos\": \"preposition\",\n",
              "      \"sense\": \"for\",\n",
              "      \"word\": \"لَهُ\"\n",
              "    },\n",
              "    {\n",
              "      \"pos\": \"noun\",\n",
              "      \"sense\": \"dry ablution\",\n",
              "      \"word\": \"التَّيَمُّمُ\"\n",
              "    },\n",
              "    {\n",
              "      \"pos\": \"verb\",\n",
              "      \"sense\": \"to be contemplated\",\n",
              "      \"word\": \"يُتَأَمَّلُ\"\n",
              "    },\n",
              "    {\n",
              "      \"pos\": \"noun\",\n",
              "      \"sense\": \"face\",\n",
              "      \"word\": \"وَجْهُ\"\n",
              "    },\n",
              "    {\n",
              "      \"pos\": \"pronoun\",\n",
              "      \"sense\": \"that\",\n",
              "      \"word\": \"ذَلِكَ\"\n",
              "    }\n",
              "  ]\n",
              "}\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_text(test_data, model, \"Bilstm_wsd\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BylSDBlVzs7b",
        "outputId": "37419899-e4f7-4229-ca0e-a9a3a391a103"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 1s 880ms/step\n",
            "1/1 [==============================] - 0s 240ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 1s 696ms/step\n",
            "1/1 [==============================] - 0s 332ms/step\n",
            "1/1 [==============================] - 0s 412ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 1s 740ms/step\n",
            "1/1 [==============================] - 1s 618ms/step\n",
            "1/1 [==============================] - 0s 408ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 371ms/step\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 576ms/step\n",
            "1/1 [==============================] - 1s 537ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 539ms/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 1s 859ms/step\n",
            "1/1 [==============================] - 1s 686ms/step\n",
            "1/1 [==============================] - 1s 797ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 1s 862ms/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 0s 176ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 361ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 386ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 394ms/step\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "1/1 [==============================] - 1s 724ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 1s 796ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 1s 756ms/step\n",
            "1/1 [==============================] - 1s 751ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 134ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 1s 501ms/step\n",
            "1/1 [==============================] - 1s 681ms/step\n",
            "1/1 [==============================] - 1s 911ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 1s 568ms/step\n",
            "1/1 [==============================] - 0s 217ms/step\n",
            "1/1 [==============================] - 0s 192ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 183ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 285ms/step\n",
            "1/1 [==============================] - 1s 605ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 377ms/step\n",
            "1/1 [==============================] - 0s 290ms/step\n",
            "1/1 [==============================] - 0s 402ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 251ms/step\n",
            "1/1 [==============================] - 1s 555ms/step\n",
            "1/1 [==============================] - 0s 210ms/step\n",
            "1/1 [==============================] - 1s 634ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 1s 616ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 1s 897ms/step\n",
            "1/1 [==============================] - 1s 748ms/step\n",
            "1/1 [==============================] - 0s 282ms/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 0s 310ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 435ms/step\n",
            "1/1 [==============================] - 0s 356ms/step\n",
            "1/1 [==============================] - 1s 749ms/step\n",
            "1/1 [==============================] - 0s 361ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 0s 159ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 1s 756ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 338ms/step\n",
            "1/1 [==============================] - 1s 965ms/step\n",
            "1/1 [==============================] - 0s 135ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 103ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 1s 514ms/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 177ms/step\n",
            "1/1 [==============================] - 0s 309ms/step\n",
            "1/1 [==============================] - 1s 676ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 1s 860ms/step\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "1/1 [==============================] - 0s 354ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 480ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 646ms/step\n",
            "1/1 [==============================] - 0s 241ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 486ms/step\n",
            "1/1 [==============================] - 1s 578ms/step\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 191ms/step\n",
            "1/1 [==============================] - 1s 710ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "1/1 [==============================] - 0s 361ms/step\n",
            "1/1 [==============================] - 0s 334ms/step\n",
            "1/1 [==============================] - 0s 361ms/step\n",
            "1/1 [==============================] - 0s 440ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 1s 732ms/step\n",
            "1/1 [==============================] - 1s 902ms/step\n",
            "1/1 [==============================] - 0s 419ms/step\n",
            "1/1 [==============================] - 1s 922ms/step\n",
            "1/1 [==============================] - 1s 702ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 352ms/step\n",
            "1/1 [==============================] - 0s 347ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 157ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "1/1 [==============================] - 1s 764ms/step\n",
            "1/1 [==============================] - 0s 260ms/step\n",
            "1/1 [==============================] - 1s 951ms/step\n",
            "1/1 [==============================] - 1s 787ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 383ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 222ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 185ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 1s 655ms/step\n",
            "1/1 [==============================] - 0s 406ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 1s 891ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 190ms/step\n",
            "1/1 [==============================] - 1s 954ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 604ms/step\n",
            "1/1 [==============================] - 1s 912ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 1s 659ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 1s 751ms/step\n",
            "1/1 [==============================] - 1s 656ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 0s 309ms/step\n",
            "1/1 [==============================] - 1s 859ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 1s 641ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "1/1 [==============================] - 0s 359ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 1s 911ms/step\n",
            "1/1 [==============================] - 1s 650ms/step\n",
            "1/1 [==============================] - 0s 189ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 201ms/step\n",
            "1/1 [==============================] - 1s 881ms/step\n",
            "1/1 [==============================] - 0s 194ms/step\n",
            "1/1 [==============================] - 0s 396ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 233ms/step\n",
            "1/1 [==============================] - 1s 902ms/step\n",
            "1/1 [==============================] - 0s 197ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 230ms/step\n",
            "1/1 [==============================] - 0s 296ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 1s 730ms/step\n",
            "1/1 [==============================] - 0s 336ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "1/1 [==============================] - 1s 577ms/step\n",
            "1/1 [==============================] - 1s 808ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 260ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 1s 688ms/step\n",
            "1/1 [==============================] - 0s 230ms/step\n",
            "1/1 [==============================] - 0s 205ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 303ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "1/1 [==============================] - 0s 117ms/step\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 1s 608ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 188ms/step\n",
            "1/1 [==============================] - 0s 371ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 287ms/step\n",
            "1/1 [==============================] - 1s 819ms/step\n",
            "1/1 [==============================] - 0s 182ms/step\n",
            "1/1 [==============================] - 1s 664ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 321ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "1/1 [==============================] - 0s 243ms/step\n",
            "1/1 [==============================] - 1s 858ms/step\n",
            "1/1 [==============================] - 0s 499ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 951ms/step\n",
            "1/1 [==============================] - 1s 655ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 341ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 159ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 216ms/step\n",
            "1/1 [==============================] - 0s 191ms/step\n",
            "1/1 [==============================] - 0s 238ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 116ms/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 1s 884ms/step\n",
            "1/1 [==============================] - 1s 879ms/step\n",
            "1/1 [==============================] - 1s 582ms/step\n",
            "1/1 [==============================] - 1s 941ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 1s 712ms/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 471ms/step\n",
            "1/1 [==============================] - 0s 161ms/step\n",
            "1/1 [==============================] - 0s 498ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 185ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 129ms/step\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 457ms/step\n",
            "1/1 [==============================] - 0s 213ms/step\n",
            "1/1 [==============================] - 0s 165ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 434ms/step\n",
            "1/1 [==============================] - 0s 313ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 493ms/step\n",
            "1/1 [==============================] - 0s 411ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 1s 580ms/step\n",
            "1/1 [==============================] - 1s 892ms/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 1s 775ms/step\n",
            "1/1 [==============================] - 1s 526ms/step\n",
            "1/1 [==============================] - 1s 739ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 801ms/step\n",
            "1/1 [==============================] - 1s 539ms/step\n",
            "1/1 [==============================] - 1s 958ms/step\n",
            "1/1 [==============================] - 0s 452ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 369ms/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "1/1 [==============================] - 0s 164ms/step\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "1/1 [==============================] - 1s 637ms/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 1s 520ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'word'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-206-b7e3547140ff>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Bilstm_wsd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-203-9ea73075ac82>\u001b[0m in \u001b[0;36mpredict_text\u001b[0;34m(data, model, file_name)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{file_name}_out.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-203-9ea73075ac82>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(line, model)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#line['sentence'] = remove_diacritics_line(line['sentence'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-195-a60007efa8d2>\u001b[0m in \u001b[0;36mget_classes\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sentence_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-194-597ad692ce56>\u001b[0m in \u001b[0;36mget_sentence_classes\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'sense'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m           \u001b[0msens_emb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_word_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sense'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'word'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9- Error Calculation"
      ],
      "metadata": {
        "id": "NsEXB7Dimr93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diacritization_evaluation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1q2fJlqKOPJ",
        "outputId": "02ed081d-412d-4f43-d2d2-9f3e44a32aee"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting diacritization_evaluation\n",
            "  Downloading diacritization_evaluation-0.5-py3-none-any.whl.metadata (945 bytes)\n",
            "Downloading diacritization_evaluation-0.5-py3-none-any.whl (7.2 kB)\n",
            "Installing collected packages: diacritization_evaluation\n",
            "Successfully installed diacritization_evaluation-0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_illegal_diac(string):\n",
        "  ss  = \"\"\n",
        "  for s in string:\n",
        "    if s in ARABIC_CHAR_SPACE:\n",
        "      ss += s\n",
        "      continue\n",
        "    if s in PUNCTUATIONS:\n",
        "      ss += s\n",
        "  return ss\n",
        "\n",
        "def calculate_der(original_path, predicted_path, case_ending=True ):\n",
        "  with open(original_path, encoding=\"utf8\") as file:\n",
        "        original_content = file.read()\n",
        "\n",
        "  with open(predicted_path, encoding=\"utf8\") as file:\n",
        "        predicted_content = file.read()\n",
        "\n",
        "  return der.calculate_der(original_content, remove_incorrect_diac(predicted_content), case_ending=case_ending)\n",
        "  # avg_der = 0\n",
        "  # number_of_sentences = 0\n",
        "  # for sentence in zip(original_content, predicted_content):\n",
        "  #   try:\n",
        "  #     avg_der += der.calculate_der(sentence[0], sentence[1], case_ending=case_ending)\n",
        "  #     number_of_sentences += 1\n",
        "  #   except:\n",
        "  #     continue\n",
        "\n",
        "  return avg_der / number_of_sentences\n",
        "\n",
        "def calculate_wer(original_path, predicted_path, case_ending=False ):\n",
        "  with open(original_path, encoding=\"utf8\") as file:\n",
        "        original_content = file.read()\n",
        "\n",
        "  with open(predicted_path, encoding=\"utf8\") as file:\n",
        "        predicted_content = file.read()\n",
        "\n",
        "  return wer.calculate_wer(original_path, predicted_path, case_ending=case_ending, include_non_arabic=True)\n",
        "  # avg_wer = 0\n",
        "  # number_of_sentences = 0\n",
        "  # for sentence in zip(original_content, predicted_content):\n",
        "  #   try:\n",
        "  #     avg_wer += wer.calculate_wer(sentence[0], sentence[1], case_ending=case_ending)\n",
        "  #     number_of_sentences += 1\n",
        "  #   except:\n",
        "  #     continue\n",
        "\n",
        "  # return avg_wer / number_of_sentences"
      ],
      "metadata": {
        "id": "4wfvFrM3Kaih"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diacritization_evaluation import wer, der\n",
        "original_path = \"/content/drive/MyDrive/ATD-WSD/Bilstm_wsd_inp.txt\"\n",
        "predicted_path  = \"/content/drive/MyDrive/ATD-WSD/Bilstm_wsd_out.txt\"\n",
        "\n",
        "print(calculate_der(original_path, predicted_path))\n",
        "print(calculate_wer(original_path, predicted_path))"
      ],
      "metadata": {
        "id": "OuhNC8sUjBwk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10b597da-963f-4132-89ce-65b01af07350"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59.72\n",
            "0.0\n"
          ]
        }
      ]
    }
  ]
}