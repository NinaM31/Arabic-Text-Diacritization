{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1- Setup Project"
      ],
      "metadata": {
        "id": "iTwVx-i4B_Dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-KshqcWmWrF",
        "outputId": "81edf9c1-6d29-4439-ef31-879a5bcae460"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/ATD-WSD\n",
        "\n",
        "# Create dir to for storing trained model\n",
        "#!mkdir Baseline-w2v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoBoeJn_mWn1",
        "outputId": "fa77dd45-f174-4a49-bc71-8a8d827879b6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ATD-WSD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.1\n",
        "!pip install tensorflow==2.14.0"
      ],
      "metadata": {
        "id": "tch6Y_RWmWkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pickle\n",
        "import json\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import joblib\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from keras.layers import Embedding, Dense, Dropout, LSTM, Bidirectional, TimeDistributed, InputLayer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import Sequence\n",
        "from keras.initializers import glorot_normal\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from rich import print_json"
      ],
      "metadata": {
        "id": "pVmQYpAWmWhy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "    print(\"GPU device not found: working on CPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqoGoZ2VmWex",
        "outputId": "71560a23-fa62-48a3-aa3d-b5593e981cf6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU device not found: working on CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2- Importing Dataset\n",
        "The dataset that was prepared using Gemini\n",
        "```\n",
        "[{\n",
        "    \"sentence\": \"some text in arabic\",\n",
        "    \"words\": [\n",
        "      {\n",
        "        \"word\": \"word_1\",\n",
        "        \"word_sense\": \"definition_1\"\n",
        "        \"pos\" : \"part_of_speech_1\"\n",
        "      }\n",
        "    ]\n",
        "}]\n",
        "```\n",
        "\n",
        "***⚠️This baseline model will use the sentences without the sense***"
      ],
      "metadata": {
        "id": "lklEh4uKCii9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helpers\n",
        "def read_json(file_path):\n",
        "  with open(file_path, mode=\"r\", encoding=\"utf-8\") as json_data:\n",
        "    return get_sentences(json.load(json_data))\n",
        "\n",
        "def get_sentences(data):\n",
        "  sentences = []\n",
        "  for s in data:\n",
        "    sentences.append(s['sentence'])\n",
        "  return sentences\n",
        "\n",
        "def pprint(json_data):\n",
        "  print_json(data=json_data, highlight=False)"
      ],
      "metadata": {
        "id": "Blt3y72LmWbr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = read_json(\"/content/10485_train_wsd.json\")\n",
        "val_data = read_json(\"/content/2517_val_wsd.json\")\n",
        "\n",
        "print('Training data length:', len(train_data))\n",
        "print(\"Train Sample\")\n",
        "print(train_data[100])\n",
        "\n",
        "print('Validation data length:', len(val_data))\n",
        "print(\"Val Sample\")\n",
        "print(val_data[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwGB9ooSmWYT",
        "outputId": "658d9831-f30a-47ca-c446-6d149caa8c29"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data length: 10485\n",
            "Train Sample\n",
            "فَاسِدٌ\n",
            "Validation data length: 2517\n",
            "Val Sample\n",
            "وَلَوْ لَمْ تَزِدْ( 26 / 106 )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3- Constants"
      ],
      "metadata": {
        "id": "G5hK3htvKgvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helpers\n",
        "def CHAR_IDX(LIST):\n",
        "    char2idx = {}\n",
        "    idx2char = {}\n",
        "\n",
        "    for i, char in enumerate(LIST):\n",
        "        char2idx[char] = i\n",
        "        idx2char[i] = char\n",
        "\n",
        "    return char2idx, idx2char"
      ],
      "metadata": {
        "id": "9sdWVjt1Qv67"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ARABIC_CHAR = \"ىعظحرسيشضقثلصطكآماإهزءأفؤغجئدةخوبذتن\"\n",
        "NUMBERS = \"0123456789٠١٢٣٤٥٦٧٨٩\"\n",
        "\n",
        "# 15 possible Diacritics\n",
        "FATHATAN = u'\\u064b'\n",
        "DAMMATAN = u'\\u064c'\n",
        "KASRATAN = u'\\u064d'\n",
        "FATHA = u'\\u064e'\n",
        "DAMMA = u'\\u064f'\n",
        "KASRA = u'\\u0650'\n",
        "SHADDA = u'\\u0651'\n",
        "SUKUN = u'\\u0652'\n",
        "\n",
        "DIACRITICS = [\n",
        "    \"\",              # No Diacritic\n",
        "    FATHA,           # Fatha\n",
        "    FATHATAN,        # Fathatah\n",
        "    DAMMA,           # Damma\n",
        "    DAMMATAN,        # Dammatan\n",
        "    KASRA,           # Kasra\n",
        "    KASRATAN,        # Kasratan\n",
        "    SUKUN,           # Sukun\n",
        "    SHADDA,          # Shadda\n",
        "    SHADDA+FATHA,    # Shadda + Fatha\n",
        "    SHADDA+FATHATAN, # Shadda + Fathatah\n",
        "    SHADDA+DAMMA,    # Shadda + Damma\n",
        "    SHADDA+DAMMATAN, # Shadda + Dammatan\n",
        "    SHADDA+KASRA,    # Shadda + Kasra\n",
        "    SHADDA+KASRATAN  # Shadda + Kasratan\n",
        "]\n",
        "\n",
        "PUNCTUATIONS = [\n",
        "    \".\",    \"،\",    \":\",    \"؛\",\n",
        "    \"-\",    \"–\",    \"«\",    \"»\",\n",
        "    \"~\",    \"؟\",    \"!\",    \"*\",\n",
        "    \"(\",    \")\",    \"[\",    \"]\",\n",
        "    \"{\",    \"}\",    \";\",    \"\\n\",\n",
        "    \"'\",    \"\\\"\",   \"`\",    \"/\",\n",
        "    \",\",    \"?\",    '’',    '“',\n",
        "    '…',    '﴾',    '﴿',    \"+\",\n",
        "    \"*\",    \"=\",    \"&\",    \"_\",\n",
        "    \"\\n\",   \"\\u200d\",       \"\\u200f\"\n",
        "]\n",
        "\n",
        "\n",
        "# Special Tokens\n",
        "UNK_TOKEN = \"<unk>\"\n",
        "PAD_TOKEN = \"<pad>\"\n",
        "SOS_TOKEN = \"<s>\"\n",
        "EOS_TOKEN = \"</s>\"\n",
        "SPECIAL_TOKENS = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN]\n",
        "\n",
        "# Combine\n",
        "ARABIC_CHAR_SPACE = list(ARABIC_CHAR) + [' ']\n",
        "ARABIC_CHAR_VALID = ARABIC_CHAR_SPACE + DIACRITICS\n",
        "ALLCHARS = ARABIC_CHAR_SPACE + list(NUMBERS) + PUNCTUATIONS + SPECIAL_TOKENS"
      ],
      "metadata": {
        "id": "smVTAeOjKggL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_mapping, reverse_char_mapping = CHAR_IDX(ALLCHARS)\n",
        "class_mapping, reverse_class_mapping = CHAR_IDX(DIACRITICS)\n",
        "\n",
        "print(\"Char Mapping Size:\", len(char_mapping))\n",
        "print(\"Class Mapping Size:\", len(class_mapping))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4RjJXjyLw2e",
        "outputId": "aad3825b-b679-429d-9a34-78a36f403d8f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Char Mapping Size: 97\n",
            "Class Mapping Size: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4- Helper Functions"
      ],
      "metadata": {
        "id": "Ge171xcVEiUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_diacritics_line(data):\n",
        "    return data.translate(str.maketrans('', '', ''.join(DIACRITICS)))\n",
        "\n",
        "def get_max_size(data):\n",
        "  return max(len(remove_diacritics_line(item).strip()) for item in data)\n",
        "\n",
        "def get_min_size(data):\n",
        "  return min(len(remove_diacritics_line(item).strip()) for item in data)\n",
        "\n",
        "def one_hot_matrix(data, size):\n",
        "    one_hot_matrix = [[1 if j == i else 0 for j in range(size)] for i in data]\n",
        "    return one_hot_matrix\n",
        "\n",
        "def one_hot_vector(index , size):\n",
        "    one_hot_vector = [1 if j == index else 0 for j in range(size)]\n",
        "    return one_hot_vector\n",
        "\n",
        "def punc_split(data):\n",
        "  split_data = list()\n",
        "  for line in data:\n",
        "    line = line.replace('.', '.\\n')\n",
        "    line = line.replace(',', ',\\n')\n",
        "    line = line.replace('،', '،\\n')\n",
        "    line = line.replace(':', ':\\n')\n",
        "    line = line.replace(';', ';\\n')\n",
        "    line = line.replace('؛', '؛\\n')\n",
        "    line = line.replace('(', '\\n(')\n",
        "    line = line.replace(')', ')\\n')\n",
        "    line = line.replace('[', '\\n[')\n",
        "    line = line.replace(']', ']\\n')\n",
        "    line = line.replace('{', '\\n{')\n",
        "    line = line.replace('}', '}\\n')\n",
        "    line = line.replace('«', '\\n«')\n",
        "    line = line.replace('»', '»\\n')\n",
        "    line = line.replace('؟', '؟\\n')\n",
        "    line = line.replace('?', '?\\n')\n",
        "    line = line.replace('!', '!\\n')\n",
        "    line = line.replace('-', '-\\n')\n",
        "\n",
        "    split_data += line.split('\\n')\n",
        "\n",
        "  return split_data\n",
        "\n",
        "def split_on_length(data, max_len=500):\n",
        "    splitted_data = list()\n",
        "\n",
        "    for sentence in punc_split(data):\n",
        "\n",
        "       new_sentence = remove_diacritics_line(sentence).strip()\n",
        "\n",
        "       if len(new_sentence) != 0:\n",
        "          if len(new_sentence) > 0 and len(new_sentence) <= max_len:\n",
        "                  splitted_data.append(sentence.strip())\n",
        "          else:\n",
        "            sentence_words = sentence.split()\n",
        "            temp_sentence = ''\n",
        "\n",
        "            for word in sentence_words:\n",
        "              if len(remove_diacritics_line(temp_sentence).strip()) + len(remove_diacritics_line(word).strip()) + 1 > max_len:\n",
        "                  if len(remove_diacritics_line(temp_sentence).strip()) > 0:\n",
        "                      splitted_data.append(temp_sentence.strip())\n",
        "                  temp_sentence = word\n",
        "\n",
        "              else:\n",
        "                  temp_sentence = word if temp_sentence == '' else temp_sentence + ' ' + word\n",
        "\n",
        "            if len(remove_diacritics_line(temp_sentence).strip()) > 0:\n",
        "                  splitted_data.append(temp_sentence.strip())\n",
        "\n",
        "    return splitted_data\n",
        "\n",
        "def train_word_embeddings(docs):\n",
        "    tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "    tokenizer.fit_on_texts(docs)\n",
        "\n",
        "    sentences = [doc.split() for doc in docs ]\n",
        "    sentences.append([UNK_TOKEN])\n",
        "    word2vec_model = Word2Vec(sentences, vector_size = 300, window=5, min_count=1, workers=4)\n",
        "\n",
        "    word_embeddings = word2vec_model.wv\n",
        "\n",
        "    return word_embeddings, tokenizer\n",
        "\n",
        "def get_word_embeddings(word):\n",
        "    encoded_docs = tokenizer.texts_to_sequences(word)\n",
        "    word_embeddings_for_sample = []\n",
        "    for word_index in encoded_docs:\n",
        "      if len(word_index) > 0:\n",
        "        if word_index[0] in data_embeddings:\n",
        "          word_embeddings_for_sample.append(data_embeddings[word_index[0]])\n",
        "\n",
        "    return word_embeddings_for_sample"
      ],
      "metadata": {
        "id": "pKp_49w2mWSl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5- Prepare Data"
      ],
      "metadata": {
        "id": "3YTSfJP5oL6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_length_train_data      = split_on_length(train_data)\n",
        "split_length_val_data      = split_on_length(val_data)\n",
        "\n",
        "print(\"Train Data Size:\", len(split_length_train_data))\n",
        "print('Training data max:', get_max_size(split_length_train_data))\n",
        "print('Training data min:', get_min_size(split_length_train_data))\n",
        "print(\"Train Sample:\", split_length_train_data[0:2])\n",
        "print()\n",
        "\n",
        "print(\"Val Data Size:\", len(split_length_val_data))\n",
        "print('Validation data max:', get_max_size(split_length_val_data))\n",
        "print('Validation data min:', get_min_size(split_length_val_data))\n",
        "print(\"Val Sample:\", split_length_val_data[0:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVqH68dByDKY",
        "outputId": "5f86fb59-1488-4b49-f863-d0eda7cae945"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Data Size: 35722\n",
            "Training data max: 500\n",
            "Training data min: 1\n",
            "Train Sample: ['وَلَوْ جَمَعَ ثُمَّ عَلِمَ تَرْكَ رُكْنٍ مِنْ الْأُولَى بَطَلَتَا وَيُعِيدُهُمَا جَامِعًا ،', 'أَوْ مِنْ الثَّانِيَةِ ،']\n",
            "\n",
            "Val Data Size: 15120\n",
            "Validation data max: 500\n",
            "Validation data min: 1\n",
            "Val Sample: ['وَقَوْلُهُ', '( وَلَوْ حَلَفَ لَا يَجْلِسُ عَلَى سَرِيرٍ )']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data without diacritics and without punc and numbers"
      ],
      "metadata": {
        "id": "hCucn8BZ8nzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_diac_train_data = [(''.join(char for char in text if char in ARABIC_CHAR_VALID)).strip() for text in split_length_train_data]\n",
        "clean_diac_val_data = [(''.join(char for char in text if char in ARABIC_CHAR_VALID)).strip() for text in split_length_val_data]\n",
        "\n",
        "clean_diac_train_data = [item for item in clean_diac_train_data if item != \"\"]\n",
        "clean_diac_val_data   = [item for item in clean_diac_val_data if item != \"\"]\n",
        "\n",
        "print('Training data length:', len(clean_diac_train_data))\n",
        "print('Validation data length:', len(clean_diac_val_data))\n",
        "\n",
        "print(clean_diac_train_data[0:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHz280qHoBuL",
        "outputId": "304a3fe1-b277-4dc7-a192-527fd8863016"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data length: 31578\n",
            "Validation data length: 13458\n",
            "['وَلَوْ جَمَعَ ثُمَّ عَلِمَ تَرْكَ رُكْنٍ مِنْ الْأُولَى بَطَلَتَا وَيُعِيدُهُمَا جَامِعًا', 'أَوْ مِنْ الثَّانِيَةِ', 'فَإِنْ لَمْ يَطُلْ تَدَارَكَ', 'وَإِلَّا فَبَاطِلَةٌ وَلَا جَمَعَ', 'وَلَوْ جَهِلَ أَعَادَهُمَا لِوَقْتَيْهِمَا']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data with diacritics and without punc and numbers"
      ],
      "metadata": {
        "id": "SEPnoVft8dyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_train_data = [remove_diacritics_line(text) for text in clean_diac_train_data]\n",
        "clean_val_data = [remove_diacritics_line(text) for text in clean_diac_val_data]\n",
        "\n",
        "print('Training data length:', len(clean_train_data))\n",
        "print('Validation data length:', len(clean_val_data))\n",
        "\n",
        "print(clean_train_data[0:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ6mag5FoBrb",
        "outputId": "4a16b0b6-7bb7-44cb-cd50-67b0e23b4c64"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data length: 31578\n",
            "Validation data length: 13458\n",
            "['ولو جمع ثم علم ترك ركن من الأولى بطلتا ويعيدهما جامعا', 'أو من الثانية']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train word embedding"
      ],
      "metadata": {
        "id": "ET_nC50K86C_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_to_embeddings = clean_train_data + clean_val_data\n",
        "data_embeddings, tokenizer = train_word_embeddings(data_to_embeddings)"
      ],
      "metadata": {
        "id": "yP1XBrwooBmO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6- Custom Data Generator"
      ],
      "metadata": {
        "id": "s8Q1H6ZKacWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_classes(sentence):\n",
        "  x = []\n",
        "  y = []\n",
        "\n",
        "  unk_emb = get_word_embeddings([UNK_TOKEN])[0]\n",
        "\n",
        "  vec = []\n",
        "  vec = one_hot_vector(char_mapping[SOS_TOKEN],len(char_mapping))\n",
        "  vec.extend(unk_emb)\n",
        "  x.append(vec)\n",
        "\n",
        "  y.append(one_hot_vector(class_mapping[''],len(class_mapping)))\n",
        "  split_sentence = [i for j in sentence.split() for i in (j, ' ')][:-1]\n",
        "\n",
        "  for word in split_sentence:\n",
        "    emb2 = get_word_embeddings(remove_diacritics_line(word))\n",
        "\n",
        "    if (len(emb2) == 0):\n",
        "        emb = unk_emb\n",
        "    else:\n",
        "        emb = emb2[0]\n",
        "\n",
        "    if word in PUNCTUATIONS:\n",
        "      emb = unk_emb\n",
        "    else:\n",
        "      if (len(emb2) == 0):\n",
        "        emb = unk_emb\n",
        "      else:\n",
        "        emb = emb2[0]\n",
        "\n",
        "    for index, char in enumerate(word):\n",
        "      if char not in DIACRITICS:\n",
        "        vec = []\n",
        "        vec = one_hot_vector(char_mapping[char],len(char_mapping))\n",
        "        vec.extend(emb)\n",
        "        x.append(vec)\n",
        "        char_diacritic = ''\n",
        "        sentence_len = len(sentence)\n",
        "\n",
        "        if index + 1 < sentence_len:\n",
        "          if sentence[index + 1] in DIACRITICS:\n",
        "            char_diacritic = sentence[index + 1]\n",
        "\n",
        "            if index + 2 < sentence_len:\n",
        "               char_diacritic = char_diacritic + sentence[index + 2] if sentence[index + 2] in DIACRITICS and (char_diacritic + sentence[index + 2] in class_mapping) else sentence[index + 2] + char_diacritic if sentence[index + 2] in DIACRITICS and (sentence[index + 2] + char_diacritic in class_mapping) else char_diacritic\n",
        "        y.append(one_hot_vector(class_mapping[char_diacritic],len(class_mapping)))\n",
        "\n",
        "  vec = []\n",
        "  vec = one_hot_vector(char_mapping[EOS_TOKEN],len(char_mapping))\n",
        "  vec.extend(unk_emb)\n",
        "  x.append(vec)\n",
        "\n",
        "  y.append(one_hot_vector(class_mapping[''],len(class_mapping)))\n",
        "\n",
        "  assert(len(x) == len(y))\n",
        "\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "b1dnZ-DeoBgd"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_classes(data):\n",
        "  X = []\n",
        "  Y = []\n",
        "\n",
        "  for sentence in data:\n",
        "    x, y = get_sentence_classes(sentence)\n",
        "    X.append(x)\n",
        "    Y.append(y)\n",
        "\n",
        "  X = np.asarray(X)\n",
        "  Y = np.asarray(Y)\n",
        "\n",
        "  return X, Y\n",
        "\n",
        "class custom_data_generator(Sequence):\n",
        "\n",
        "    def __init__(self, data, batch_size):\n",
        "        self.data = data\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.data) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        start_index = index * self.batch_size\n",
        "        end_index = (index + 1) * self.batch_size\n",
        "\n",
        "        batch = self.data[start_index : end_index]\n",
        "        X_batch, Y_batch = get_classes(batch)\n",
        "\n",
        "        max_length_X = np.max([len(x) for x in X_batch])\n",
        "        max_length_Y = np.max([len(y) for y in Y_batch])\n",
        "\n",
        "        assert(max_length_X == max_length_Y)\n",
        "\n",
        "        vec = []\n",
        "        vec = one_hot_vector(char_mapping[PAD_TOKEN],len(char_mapping))\n",
        "        vec.extend(get_word_embeddings([PAD_TOKEN])[0])\n",
        "\n",
        "        X = []\n",
        "        for x in X_batch:\n",
        "          padding_length = max_length_X - len(x)\n",
        "          x = list(x)\n",
        "          x.extend([vec] * (padding_length))\n",
        "          X.append(np.asarray(x))\n",
        "\n",
        "        Y = []\n",
        "        for y in Y_batch:\n",
        "          padding_length = max_length_Y - len(y)\n",
        "          y = list(y)\n",
        "\n",
        "          y.extend(one_hot_matrix([class_mapping['']] * (padding_length), len(class_mapping)))\n",
        "          Y.append(np.asarray(y))\n",
        "\n",
        "        X, Y = np.asarray(X), np.asarray(Y)\n",
        "        return X, Y"
      ],
      "metadata": {
        "id": "8K_GCQrRmWAs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "   model = Sequential()\n",
        "   model.add(InputLayer(input_shape=(None, 397)))\n",
        "\n",
        "   model.add(Bidirectional(LSTM(units=256,return_sequences=True,kernel_initializer=glorot_normal(seed=500))))\n",
        "   model.add(Dropout(0.5))\n",
        "   model.add(Bidirectional(LSTM(units=256,return_sequences=True,kernel_initializer=glorot_normal(seed=500))))\n",
        "   model.add(Dropout(0.5))\n",
        "   model.add(Bidirectional(LSTM(units=256,return_sequences=True,kernel_initializer=glorot_normal(seed=500))))\n",
        "   model.add(TimeDistributed(Dense(units=512,activation='relu',kernel_initializer=glorot_normal(seed=500))))\n",
        "   model.add(TimeDistributed(Dense(units=512,activation='relu',kernel_initializer=glorot_normal(seed=500))))\n",
        "   model.add(TimeDistributed(Dense(units=len(class_mapping),activation='softmax',kernel_initializer=glorot_normal(seed=500))))\n",
        "   model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "   return model"
      ],
      "metadata": {
        "id": "Y2soLOb3ohZx"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOMzl4wAohRq",
        "outputId": "f1de0506-8bc5-4a82-f795-4dd20dca2a99"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional (Bidirection  (None, None, 512)         1339392   \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, 512)         0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, None, 512)         1574912   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, None, 512)         0         \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, None, 512)         1574912   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " time_distributed (TimeDist  (None, None, 512)         262656    \n",
            " ributed)                                                        \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDi  (None, None, 512)         262656    \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_2 (TimeDi  (None, None, 15)          7695      \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5022223 (19.16 MB)\n",
            "Trainable params: 5022223 (19.16 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_model(model, epochs, batch_size, train_data, val_data):\n",
        "    random.shuffle(train_data)\n",
        "    random.shuffle(val_data)\n",
        "\n",
        "    train_data = list(sorted(train_data, key=lambda item: len(remove_diacritics_line(item))))\n",
        "    val_data   = list(sorted(val_data,   key=lambda item: len(remove_diacritics_line(item))))\n",
        "\n",
        "    checkpoint_path = '/content/drive/MyDrive/ATD-WSD/Baseline-w2v/epoch{epoch:02d}.ckpt'\n",
        "\n",
        "    checkpoint_cb = ModelCheckpoint(checkpoint_path, verbose=0)\n",
        "\n",
        "    training_generator = custom_data_generator(train_data, batch_size)\n",
        "    val_generator = custom_data_generator(val_data, batch_size)\n",
        "\n",
        "    history =  model.fit(training_generator,validation_data=val_generator,epochs=epochs,callbacks=[checkpoint_cb])\n",
        "    return history"
      ],
      "metadata": {
        "id": "bgf8wJQvomsF"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = fit_model(model, 5, 256, clean_diac_train_data, clean_diac_val_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-w9KIkNcomoM",
        "outputId": "c7481d0a-36b4-41ce-eb4f-8bcb610bc4bf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-22-4820fd75929e>:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.asarray(X)\n",
            "<ipython-input-22-4820fd75929e>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  Y = np.asarray(Y)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "124/124 [==============================] - 924s 7s/step - loss: 1.3425 - accuracy: 0.4848 - val_loss: 1.1553 - val_accuracy: 0.5949\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-22-4820fd75929e>:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.asarray(X)\n",
            "<ipython-input-22-4820fd75929e>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  Y = np.asarray(Y)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "124/124 [==============================] - 912s 7s/step - loss: 1.0424 - accuracy: 0.6334 - val_loss: 0.9869 - val_accuracy: 0.6568\n",
            "Epoch 3/5\n",
            "  1/124 [..............................] - ETA: 5:11 - loss: 0.7551 - accuracy: 0.8431"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-22-4820fd75929e>:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.asarray(X)\n",
            "<ipython-input-22-4820fd75929e>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  Y = np.asarray(Y)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "124/124 [==============================] - 904s 7s/step - loss: 0.9178 - accuracy: 0.6787 - val_loss: 0.8834 - val_accuracy: 0.6941\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-22-4820fd75929e>:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.asarray(X)\n",
            "<ipython-input-22-4820fd75929e>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  Y = np.asarray(Y)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "124/124 [==============================] - 901s 7s/step - loss: 0.8158 - accuracy: 0.6965 - val_loss: 0.7499 - val_accuracy: 0.7287\n",
            "Epoch 5/5\n",
            "  2/124 [..............................] - ETA: 3:00 - loss: 0.6629 - accuracy: 0.7327"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-4820fd75929e>:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  X = np.asarray(X)\n",
            "<ipython-input-22-4820fd75929e>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  Y = np.asarray(Y)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "124/124 [==============================] - 892s 7s/step - loss: 0.7404 - accuracy: 0.7342 - val_loss: 0.6693 - val_accuracy: 0.7811\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7- Checkpoint"
      ],
      "metadata": {
        "id": "ewXj0HZCKjMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(model, 'baseline.joblib')\n",
        "filename = 'baseline.sav'\n",
        "pickle.dump(model, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "mpKJRuWNomfK"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8- Predict"
      ],
      "metadata": {
        "id": "0u5_tScRK3_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(line, model):\n",
        "    line = remove_diacritics_line(line)\n",
        "\n",
        "    X, _ = get_classes([line])\n",
        "    predictions = model.predict(X).squeeze()\n",
        "\n",
        "    output = ''\n",
        "    for char, prediction in zip(line, predictions):\n",
        "        output += char\n",
        "        if char not in ARABIC_CHAR:\n",
        "            print(char)\n",
        "            continue\n",
        "        output += reverse_class_mapping[np.argmax(prediction)]\n",
        "    return output\n",
        "\n",
        "def predict_text(data, model, file_name):\n",
        "  for idx, line in enumerate(data):\n",
        "    output = predict(line, model)\n",
        "    with open(f\"{file_name}_out.txt\", 'a') as file:\n",
        "      file.write(output + \"\\n\")\n",
        "\n",
        "    with open(f\"{file_name}_inp.txt\", 'a') as file:\n",
        "      file.write(line + \"\\n\")"
      ],
      "metadata": {
        "id": "2pfEp6gNbCcU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_file_path = 'baseline.joblib'\n",
        "model = joblib.load(model_file_path)"
      ],
      "metadata": {
        "id": "meOKesGJmV5l"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_word = \"وَلَوْ جَمَعَ ثُمَّ عَلِمَ تَرْكَ رُكْنٍ مِنْ الْأُولَى بَطَلَتَا وَيُعِيدُهُمَا جَامِعًا\""
      ],
      "metadata": {
        "id": "PmdMg1Z2vR6e"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = predict(test_word, model)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rvugSqObh-u",
        "outputId": "edaed390-9f62-4960-efb0-25d909ef3a52"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "ولَوَ جَمَعَ ثَمَ عَلَمَ تَرَكَ رَكَنَ مَنَ اَلَأولى بَطَلَتْا وَيَعَيْدهما جَاَمَعْا\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = read_json(\"/content/2528_test_wsd.json\")\n",
        "\n",
        "print('Testing data length:', len(test_data))\n",
        "print(\"Test Sample\")\n",
        "print(test_data[100])"
      ],
      "metadata": {
        "id": "J1H_mVk4yS_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_test_data = [(''.join(char for char in text if char in list(ARABIC_CHAR))).strip() for text in test_data]"
      ],
      "metadata": {
        "id": "BylSDBlVzs7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TLuoPxkxyIuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9- Error Calculation"
      ],
      "metadata": {
        "id": "NsEXB7Dimr93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diacritization_evaluation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhR5YDKOmv2z",
        "outputId": "30668366-4d86-4f60-c6b8-8c90b3915981"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting diacritization_evaluation\n",
            "  Downloading diacritization_evaluation-0.5-py3-none-any.whl.metadata (945 bytes)\n",
            "Downloading diacritization_evaluation-0.5-py3-none-any.whl (7.2 kB)\n",
            "Installing collected packages: diacritization_evaluation\n",
            "Successfully installed diacritization_evaluation-0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from diacritization_evaluation import wer, der\n",
        "original_text = \"وَلَوْ جَمَعَ ثُمَّ عَلِمَ تَرْكَ رُكْنٍ مِنْ الْأُولَى بَطَلَتَا وَيُعِيدُهُمَا جَامِعًا\"\n",
        "predicted_text = \"ولَوَ جَمَعَ ثَمَ عَلَمَ تَرَكَ رَكَنَ مَنَ اَلَأولى بَطَلَتْا وَيَعَيْدهما جَاَمَعْا\"\n",
        "\n",
        "print(der.calculate_der(original_text, predicted_text, case_ending=False))\n",
        "print(wer.calculate_wer(original_text, predicted_text, case_ending=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlSiT97dmxbR",
        "outputId": "54da491e-8af1-4508-d314-d795428f8d2d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50.0\n",
            "90.91\n"
          ]
        }
      ]
    }
  ]
}