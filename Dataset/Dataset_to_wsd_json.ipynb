{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCE4t1abRj4K"
      },
      "source": [
        "# Dataset\n",
        "Getting dataset from: [Benchmark Arabic text diacritization dataset](https://github.com/AliOsm/arabic-text-diacritization/tree/master)\n",
        "- train.txt: Contains 50,000 lines of diacritized Arabic text which can be used as training dataset\n",
        "- val.txt: Contains 2,500 lines of diacritized Arabic text which can be used as validation dataset\n",
        "- test.txt: Contains 2,500 lines of diacritized Arabic text which can be used as testing dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qqfHAZ0AYl-p"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/AliOsm/arabic-text-diacritization/refs/heads/master/dataset/train.txt &> /dev/null\n",
        "!wget https://raw.githubusercontent.com/AliOsm/arabic-text-diacritization/refs/heads/master/dataset/test.txt &> /dev/null\n",
        "!wget https://raw.githubusercontent.com/AliOsm/arabic-text-diacritization/refs/heads/master/dataset/val.txt &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5E_4PVR0ZbZd"
      },
      "outputs": [],
      "source": [
        "def read_file_content(file_path):\n",
        "    return open(file_path, encoding=\"utf8\").read()\n",
        "\n",
        "# Read and split data based on lines\n",
        "train_data = read_file_content(\"/content/train.txt\").splitlines()\n",
        "val_data = read_file_content(\"/content/val.txt\").splitlines()\n",
        "test_data = read_file_content(\"/content/test.txt\").splitlines()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi1xc9yBSqPJ"
      },
      "source": [
        "# Preprocessing dataset\n",
        "The Gemini API will act as the word sense disambugator module. It will provide the definition of each word based on it's context.\n",
        "The prompt that will be used:\n",
        "\n",
        "\n",
        "> Assume the role of an Arabic language expert, you know the definition of words in a given context.\n",
        "I'm going to provide you with a list of sentences, and for each sentence\n",
        "provide me a list of words and their word sense in arabic language and part of speech.\n",
        "Return the response as json\n",
        "List of sentences: [List of sentences]\n",
        "\n",
        "The dataset will be in this format\n",
        "```\n",
        "[{\n",
        "    \"sentence\": \"some text in arabic\",\n",
        "    \"words\": [\n",
        "      {\n",
        "        \"word\": \"word_1\",\n",
        "        \"word_sense\": \"definition_1\"\n",
        "        \"pos\" : \"part of speech\"\n",
        "      }\n",
        "    ]\n",
        "}]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsNHKHnFnvcC",
        "outputId": "fca3b086-5e6d-40e1-99ae-a1981a818509",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzufOnf9ojaV",
        "outputId": "6b3df84a-5e4f-4e32-f0fe-645cf94241fd",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ATD-WSD\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/ATD-WSD\n",
        "\n",
        "# Create dir to store the data in drive\n",
        "!mkdir train_wsd\n",
        "!mkdir val_wsd\n",
        "!mkdir test_wsd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemini Limits\n",
        "- 15 RPM (requests per minute)\n",
        "- 1,500 RPD (requests per day)\n",
        "- 8192 Output number of tokens\n",
        "- Saftey settings terminate call\n",
        "- 1 token = 4 characters\n"
      ],
      "metadata": {
        "id": "toVv-1vJWpvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The biggest issue is the max output length from the model, therefore I will split the dataset based on the tokens, and data with large tokens will be processed in a separate API call"
      ],
      "metadata": {
        "id": "AaXKmQC2AJam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def split_data_by_char_length(data, token_threshold=100):\n",
        "  '''\n",
        "    Args:\n",
        "      data: list[str] -> list of strings\n",
        "      token_threshold: int -> max number of tokens. 1 token = 4 characters\n",
        "    Returns:\n",
        "      list[str], list[str] -> acc_threshold_data, rej_threshold_data\n",
        "  '''\n",
        "  acc_threshold_data = []\n",
        "  rej_threshold_data = []\n",
        "  for sentence in data:\n",
        "    if math.ceil(len(sentence)/4) < token_threshold:\n",
        "      acc_threshold_data.append(sentence)\n",
        "    else:\n",
        "      rej_threshold_data.append(sentence)\n",
        "\n",
        "  return acc_threshold_data, rej_threshold_data"
      ],
      "metadata": {
        "id": "ooLalx1JWpho"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_threshold = 100\n",
        "\n",
        "# Train split\n",
        "train_small_token, train_large_token = split_data_by_char_length(train_data, token_threshold=token_threshold)\n",
        "print(f\"train_small_token: {len(train_small_token)} with max token length: {token_threshold}\")\n",
        "print(f\"train_large_token: {len(train_large_token)} exceeds token length: {token_threshold}\")\n",
        "\n",
        "# val split\n",
        "val_small_token, val_large_token = split_data_by_char_length(val_data, token_threshold=token_threshold)\n",
        "print(f\"val_small_token: {len(val_small_token)} with max token length: {token_threshold}\")\n",
        "print(f\"val_small_token: {len(val_large_token)} exceeds token length: {token_threshold}\")\n",
        "\n",
        "\n",
        "# test split\n",
        "test_small_token, test_large_token = split_data_by_char_length(test_data, token_threshold=token_threshold)\n",
        "print(f\"test_small_token: {len(test_small_token)} with max token length: {token_threshold}\")\n",
        "print(f\"test_small_token: {len(test_large_token)} exceeds token length: {token_threshold}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvVwqfck5iPp",
        "outputId": "bf52e005-0a35-402d-bd80-0efcc937f078"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_small_token: 34299 with max token length: 100\n",
            "train_large_token: 15701 exceeds token length: 100\n",
            "val_small_token: 1768 with max token length: 100\n",
            "val_small_token: 732 exceeds token length: 100\n",
            "test_small_token: 1700 with max token length: 100\n",
            "test_small_token: 800 exceeds token length: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download generative ai, I will use gemini flash"
      ],
      "metadata": {
        "id": "5u88MlwYCbbT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nLbkJK4havjz"
      },
      "outputs": [],
      "source": [
        "# Downloading generative ai\n",
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-fWYnZbvbRFq"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the accepted JSON response from Gemini"
      ],
      "metadata": {
        "id": "fdlAtslmBFpq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MkC67LTwBHAk"
      },
      "outputs": [],
      "source": [
        "import typing_extensions as typing\n",
        "\n",
        "class Words(typing.TypedDict):\n",
        "    word: str\n",
        "    sense: str\n",
        "    pos: str\n",
        "\n",
        "class WSD(typing.TypedDict):\n",
        "    sentence: str\n",
        "    words: list[Words]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract word sense"
      ],
      "metadata": {
        "id": "GV7_JYIxGwcg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3BNmgRouyYv1"
      },
      "outputs": [],
      "source": [
        "from ast import Global\n",
        "import json\n",
        "import time\n",
        "\n",
        "prev_failed_char_token = 9999\n",
        "\n",
        "def save_error_log(name, start_idx, end_idx):\n",
        "  with open(f\"{name}_error_log\", mode=\"a\") as f:\n",
        "    f.write(f\"{start_idx},{end_idx}\\n\")\n",
        "\n",
        "def save_failed_text(name, start_idx, end_idx, text):\n",
        "  with open(f\"{name}_failed_text_log\", mode=\"a\") as f:\n",
        "    f.write(f\"{start_idx},{end_idx},{text}\\n\")\n",
        "\n",
        "def extract_word_sense(prompt, name, batch_itr, start_idx, end_idx, total_char_tokens):\n",
        "  global prev_failed_char_token\n",
        "\n",
        "  # limited RPD: Check if this token size will cause the bad response,\n",
        "  if total_char_tokens >= prev_failed_char_token:\n",
        "    print(f\"prompt length of same size previously failed; batch stored in error_log to avoid fail request: batch_number={batch_itr}, start_idx={start_idx}, end_idx={end_idx}\")\n",
        "    save_error_log(name, start_idx, end_idx)\n",
        "    return # skip this batch\n",
        "\n",
        "  result = model.generate_content(\n",
        "      prompt,\n",
        "      # Force Gemini to respond in JSON format\n",
        "      generation_config=genai.GenerationConfig(\n",
        "          response_mime_type=\"application/json\", response_schema=list[WSD]\n",
        "      ),\n",
        "      # Gemini terminates if there is safty issues, hence ignore (BLOCK_NONE).\n",
        "      safety_settings=[\n",
        "        {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "        {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "        {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
        "        {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"}\n",
        "      ]\n",
        "    )\n",
        "\n",
        "  # Check response\n",
        "  if len(result.candidates) <= 0:\n",
        "    print(f\"No response returned: batch_number={batch_itr}, start_idx={start_idx}, end_idx={end_idx}\")\n",
        "    print(result)\n",
        "    save_error_log(name, start_idx, end_idx)\n",
        "    raise SystemExit('Stop code execution')\n",
        "  else:\n",
        "    # If request returned but not 'Natural stop'\n",
        "    if result.candidates[0].finish_reason.name != \"STOP\":\n",
        "      print(f\"Bad response: batch_number={batch_itr}, start_idx={start_idx}, end_idx={end_idx}\")\n",
        "      print(f\"Reason: {result.candidates[0].finish_reason} \\| Continue with next batch\")\n",
        "      save_error_log(name, start_idx, end_idx)\n",
        "      return # skip this batch\n",
        "\n",
        "\n",
        "  # Store response in file\n",
        "  try:\n",
        "    result_dict = json.loads(result.text)\n",
        "    with open(f\"{name}/{name}_{start_idx}_{end_idx}.json\", mode=\"w\", encoding=\"utf-8\") as f:\n",
        "      json.dump(result_dict, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "  # Incase of an error store the batch that failed\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    save_failed_text(name, start_idx, end_idx, result.text)\n",
        "    print(f\"Failed to parse: batch_number={batch_itr}, start_idx={start_idx}, end_idx={end_idx}, total_tokens={total_char_tokens}\")\n",
        "    # Store the input token size that causes bad request\n",
        "    if total_char_tokens > 600:\n",
        "      if total_char_tokens < prev_failed_char_token:\n",
        "        prev_failed_char_token = total_char_tokens\n",
        "\n",
        "    save_error_log(name, start_idx, end_idx)\n",
        "\n",
        "\n",
        "def prepare_dataset(data, data_start, data_end, name, batch_size):\n",
        "    '''\n",
        "      Args:\n",
        "        data: list[str] -> list of strings\n",
        "        data_start: int -> index for the start of the data\n",
        "        data_end: int -> index for the end of the data\n",
        "        name: str -> name of the file to store results\n",
        "        batch_size: int -> size of the batch to process\n",
        "    '''\n",
        "    batch_data = data[data_start:data_end]\n",
        "\n",
        "    start = data_start\n",
        "    end = start + batch_size\n",
        "\n",
        "    number_of_calls = math.ceil(len(batch_data)/batch_size)\n",
        "\n",
        "    # Stats related variables\n",
        "    avg_res_time = 0\n",
        "    total_time = 0\n",
        "    for i in range(number_of_calls):\n",
        "      if end > data_end:\n",
        "        end = data_end\n",
        "\n",
        "      prompt = f\"\"\"\n",
        "        You are an Arabic language expert, you understand the definition of words in a given context.\n",
        "        I will provide you with a list of sentences. For each sentence, please provide\n",
        "        a list of words, their part of speech and their word sense.\n",
        "        Return the response as json keep word sense in english.\n",
        "        List of sentences:\n",
        "        {data[start:end]}\n",
        "      \"\"\"\n",
        "      start_time = time.time()\n",
        "      extract_word_sense(\n",
        "          prompt,\n",
        "          name=name,\n",
        "          batch_itr=i,\n",
        "          start_idx=start,\n",
        "          end_idx=end,\n",
        "          total_char_tokens= math.ceil(len(prompt)/4)\n",
        "          )\n",
        "      end_time = time.time()\n",
        "      total_time += (end_time - start_time)\n",
        "\n",
        "      start = end\n",
        "      end = end + batch_size\n",
        "\n",
        "      # Print the stats\n",
        "      if (i+1) % 10 == 0:\n",
        "        avg_res_time = total_time/(i+1)\n",
        "        print(f\"Current Batch:{end} \\| iteration: {i+1}/{number_of_calls} \\| avg response time: {avg_res_time:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LG-F8a4wbsfq"
      },
      "outputs": [],
      "source": [
        "# Sample result\n",
        "prepare_dataset(train_large_token, data_start=0, data_end=1, name=\"train_wsd\", batch_size=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try with new batch size on the error log"
      ],
      "metadata": {
        "id": "wt3TIQNfklEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "\n",
        "def get_failed_slices(file_name):\n",
        "  failed_lines = []\n",
        "  with open(file_name) as f:\n",
        "    failed_lines = f.read().splitlines()\n",
        "\n",
        "  # clear file\n",
        "  with open(file_name, 'w'): pass\n",
        "\n",
        "  return failed_lines"
      ],
      "metadata": {
        "id": "AVFEcMVakkn2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "failed_lines = get_failed_slices(\"train_wsd_error_log\")\n",
        "\n",
        "for line in failed_lines:\n",
        "  start_idx, end_idx = line.split(\",\")\n",
        "  prepare_dataset(train_large_token, data_start=int(start_idx), data_end=int(end_idx), name=\"train_wsd\", batch_size=2)"
      ],
      "metadata": {
        "id": "s0-7ZiEtm9V0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Script for joining json files"
      ],
      "metadata": {
        "id": "4HzoFWqQaz4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_json_files(folder_name):\n",
        "  cwd = os.getcwd()\n",
        "  my_folder = f\"{cwd}/{folder_name}\"\n",
        "\n",
        "  json_files = [my_folder + \"/\" + f for f in listdir(my_folder) if isfile(join(my_folder, f))]\n",
        "\n",
        "  print(f\"Number of files in {folder_name}: {len(json_files)}\")\n",
        "  return json_files\n",
        "\n",
        "def join_jsons(files, output_file_name):\n",
        "  result = {}\n",
        "  for f1 in files:\n",
        "    with open(f1, 'r') as infile:\n",
        "      print(f1)\n",
        "      loaded_file = json.load(infile)\n",
        "      for d in loaded_file:\n",
        "        if d[\"sentence\"] not in result:\n",
        "          result[d[\"sentence\"]] = d\n",
        "\n",
        "  with open(f'{output_file_name}.json', 'w') as output_file:\n",
        "      dump_result = [v for v in result.values()]\n",
        "      json.dump(dump_result, output_file, ensure_ascii=False, indent=2)\n",
        "\n",
        "  cwd = os.getcwd()\n",
        "  print(f\"Joined in {cwd}/{output_file_name}\")\n"
      ],
      "metadata": {
        "id": "TrMYVInaOPLp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Join Json files"
      ],
      "metadata": {
        "id": "USnPreTBn40r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train data"
      ],
      "metadata": {
        "id": "3Y-0cZn0rM2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_folder = \"train_wsd\"\n",
        "train_wsd_jsons = get_json_files(train_folder)\n",
        "\n",
        "# Join json files\n",
        "join_jsons(train_wsd_jsons, \"220_train_wsd\")"
      ],
      "metadata": {
        "id": "zpiGZAZYa7K8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Script for checking out the data"
      ],
      "metadata": {
        "id": "bzHiWU2c2hVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "files = get_json_files(\"train_wsd\")\n",
        "\n",
        "def get_start_end_idx(file_name):\n",
        "  file_name = file_name.split(\".\")[0]\n",
        "  file_name_chopped = file_name.split(\"_\")\n",
        "  return (int(file_name_chopped[3]), int(file_name_chopped[4]))\n",
        "\n",
        "\n",
        "def get_start_idx(file_name):\n",
        "  return get_start_end_idx(file_name)[0]\n",
        "\n",
        "\n",
        "files = sorted(files, key=get_start_idx)\n",
        "for i in range(len(files)):\n",
        "  if i + 1 < len(files):\n",
        "    f = files[i]\n",
        "    f_next = files[i + 1]\n",
        "    f_start, f_end = get_start_end_idx(f)\n",
        "    f_next_start, f_next_end = get_start_end_idx(f_next)\n",
        "    if f_end != f_next_start:\n",
        "      f = f[40:]\n",
        "      f_next = f_next[40:]\n",
        "      print(f\"overlap happened between these two files: {f} - {f_next}\")\n"
      ],
      "metadata": {
        "id": "FAInveHIrUgD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92bcd123-9ec0-4b6d-dc49-abf1bc7d898b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in train_wsd: 44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "files = get_json_files(\"train_wsd\")\n",
        "number_of_sentences = 0\n",
        "files = sorted(files, key=get_start_idx)\n",
        "\n",
        "unique_sentences = set()\n",
        "duplicated_sentences = {}\n",
        "for f_name in files:\n",
        "  with open(f_name, mode=\"r\", encoding=\"utf-8\") as f:\n",
        "    sentences = json.load(f)\n",
        "    for s in sentences:\n",
        "      if s[\"sentence\"] in unique_sentences:\n",
        "        if f_name not in duplicated_sentences:\n",
        "          duplicated_sentences[f_name] = [s[\"sentence\"]]\n",
        "        else:\n",
        "          duplicated_sentences[f_name].append(s[\"sentence\"])\n",
        "      else:\n",
        "        unique_sentences.add(s[\"sentence\"])\n",
        "\n",
        "    number_of_sentences += len(sentences)\n",
        "\n",
        "print(number_of_sentences)\n",
        "\n",
        "for f_name, sentence in duplicated_sentences.items():\n",
        "  print(f_name)\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kp41dj1b0Hce",
        "outputId": "7918f2ed-0321-4809-92ba-49a5aeb0f87a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in train_wsd: 44\n",
            "220\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}